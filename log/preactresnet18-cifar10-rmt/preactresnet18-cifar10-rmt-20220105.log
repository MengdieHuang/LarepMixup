

---------------------------------------
Torch cuda is available
args.subcommand=run, run the command line
date: 20220105
whitebox attack
Experiment result save dir: /home/maggie/mmat/result/defense/rmt/fgsm/basemixup-betasampler/preactresnet18-cifar10/whitebox/20220105/00000
initilize the dataset loading parameters
load cifar10 dataset
Loading *cifar10* train dataloader finished !
Loading *cifar10* test dataloader finished !
initlize classifier
learned calssify model != None
args.adv_datasetï¼š /home/maggie/mmat/result/attack/fgsm/preactresnet18-cifar10/20210927/00000-attackacc-14.71/attack-cifar10-dataset/samples
Accuary of before rmt trained classifier on clean testset:87.3700%
Loss of before mmat trained classifier clean testset:0.4629557728767395
args.mix_mode: basemixup
args.mix_w_num: 2
args.beta_alpha: 0.5
args.dirichlet_gama: 1
cle_w_train.shape: torch.Size([25397, 8, 512])
cle_y_train.shape: torch.Size([25397, 8])
cle_x_test.shape: torch.Size([10000, 3, 32, 32])
cle_y_test.shape: torch.Size([10000])
adv_x_test.shape: torch.Size([10000, 3, 32, 32])
adv_y_test.shape: torch.Size([10000])
initlize attack classifier
generate pixel adversarial exampels
Get FGSM examples generate model
self._args.attack_eps: 0.3
generating testset adversarial examples...
finished generate testset adversarial examples !
Accuary of before rmt trained classifier on adversarial testset:14.7100%
Loss of before mmat trained classifier on adversarial testset:6.937891483306885
w_trainset_len: 25397
batch_size: 256
w_batch_num: 100


0epoch learning rate:0.001
Setting up PyTorch plugin "bias_act_plugin"... /home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/utils/cpp_extension.py:286: UserWarning: 

                               !! WARNING !!

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Your compiler (c++) is not compatible with the compiler Pytorch was
built with for this platform, which is g++ on linux. Please
use g++ to to compile your extension. Alternatively, you may
compile PyTorch from source using c++, and then you can also use
c++ to compile your extension.

See https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md for help
with compiling PyTorch from source.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

                              !! WARNING !!

  platform=sys.platform))
Done.
Setting up PyTorch plugin "upfirdn2d_plugin"... /home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/utils/cpp_extension.py:286: UserWarning: 

                               !! WARNING !!

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Your compiler (c++) is not compatible with the compiler Pytorch was
built with for this platform, which is g++ on linux. Please
use g++ to to compile your extension. Alternatively, you may
compile PyTorch from source using c++, and then you can also use
c++ to compile your extension.

See https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md for help
with compiling PyTorch from source.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

                              !! WARNING !!

  platform=sys.platform))
Done.
[Epoch 1/40] [Batch 1/196] [Batch classify loss: 1.654669]
[Epoch 1/40] [Batch 2/196] [Batch classify loss: 1.290653]
[Epoch 1/40] [Batch 3/196] [Batch classify loss: 1.143249]
[Epoch 1/40] [Batch 4/196] [Batch classify loss: 0.971766]
[Epoch 1/40] [Batch 5/196] [Batch classify loss: 1.031601]
[Epoch 1/40] [Batch 6/196] [Batch classify loss: 1.014472]
[Epoch 1/40] [Batch 7/196] [Batch classify loss: 1.117524]
[Epoch 1/40] [Batch 8/196] [Batch classify loss: 0.995627]
[Epoch 1/40] [Batch 9/196] [Batch classify loss: 1.059193]
[Epoch 1/40] [Batch 10/196] [Batch classify loss: 1.059383]
[Epoch 1/40] [Batch 11/196] [Batch classify loss: 1.008316]
[Epoch 1/40] [Batch 12/196] [Batch classify loss: 0.974889]
[Epoch 1/40] [Batch 13/196] [Batch classify loss: 0.951430]
[Epoch 1/40] [Batch 14/196] [Batch classify loss: 1.072930]
[Epoch 1/40] [Batch 15/196] [Batch classify loss: 0.974787]
[Epoch 1/40] [Batch 16/196] [Batch classify loss: 0.889910]
[Epoch 1/40] [Batch 17/196] [Batch classify loss: 0.966971]
[Epoch 1/40] [Batch 18/196] [Batch classify loss: 0.902164]
[Epoch 1/40] [Batch 19/196] [Batch classify loss: 0.966024]
[Epoch 1/40] [Batch 20/196] [Batch classify loss: 0.984941]
[Epoch 1/40] [Batch 21/196] [Batch classify loss: 0.950632]
[Epoch 1/40] [Batch 22/196] [Batch classify loss: 0.852206]
[Epoch 1/40] [Batch 23/196] [Batch classify loss: 0.907607]
[Epoch 1/40] [Batch 24/196] [Batch classify loss: 0.935266]
[Epoch 1/40] [Batch 25/196] [Batch classify loss: 0.923248]
[Epoch 1/40] [Batch 26/196] [Batch classify loss: 0.893260]
[Epoch 1/40] [Batch 27/196] [Batch classify loss: 0.934323]
[Epoch 1/40] [Batch 28/196] [Batch classify loss: 0.885741]
[Epoch 1/40] [Batch 29/196] [Batch classify loss: 0.906217]
[Epoch 1/40] [Batch 30/196] [Batch classify loss: 0.984243]
[Epoch 1/40] [Batch 31/196] [Batch classify loss: 0.914448]
[Epoch 1/40] [Batch 32/196] [Batch classify loss: 0.929723]
[Epoch 1/40] [Batch 33/196] [Batch classify loss: 0.980621]
[Epoch 1/40] [Batch 34/196] [Batch classify loss: 0.968325]
[Epoch 1/40] [Batch 35/196] [Batch classify loss: 0.842366]
[Epoch 1/40] [Batch 36/196] [Batch classify loss: 0.936799]
[Epoch 1/40] [Batch 37/196] [Batch classify loss: 0.834412]
[Epoch 1/40] [Batch 38/196] [Batch classify loss: 0.878209]
[Epoch 1/40] [Batch 39/196] [Batch classify loss: 0.863998]
[Epoch 1/40] [Batch 40/196] [Batch classify loss: 0.941234]
[Epoch 1/40] [Batch 41/196] [Batch classify loss: 0.910235]
[Epoch 1/40] [Batch 42/196] [Batch classify loss: 0.917751]
[Epoch 1/40] [Batch 43/196] [Batch classify loss: 0.883649]
[Epoch 1/40] [Batch 44/196] [Batch classify loss: 0.935569]
[Epoch 1/40] [Batch 45/196] [Batch classify loss: 0.803110]
[Epoch 1/40] [Batch 46/196] [Batch classify loss: 0.837048]
[Epoch 1/40] [Batch 47/196] [Batch classify loss: 0.881669]
[Epoch 1/40] [Batch 48/196] [Batch classify loss: 0.871272]
[Epoch 1/40] [Batch 49/196] [Batch classify loss: 0.902856]
[Epoch 1/40] [Batch 50/196] [Batch classify loss: 0.876298]
[Epoch 1/40] [Batch 51/196] [Batch classify loss: 0.848003]
[Epoch 1/40] [Batch 52/196] [Batch classify loss: 0.744145]
[Epoch 1/40] [Batch 53/196] [Batch classify loss: 0.898985]
[Epoch 1/40] [Batch 54/196] [Batch classify loss: 0.818870]
[Epoch 1/40] [Batch 55/196] [Batch classify loss: 0.856727]
[Epoch 1/40] [Batch 56/196] [Batch classify loss: 0.810918]
[Epoch 1/40] [Batch 57/196] [Batch classify loss: 0.839270]
[Epoch 1/40] [Batch 58/196] [Batch classify loss: 0.976483]
[Epoch 1/40] [Batch 59/196] [Batch classify loss: 0.927544]
[Epoch 1/40] [Batch 60/196] [Batch classify loss: 0.844098]
[Epoch 1/40] [Batch 61/196] [Batch classify loss: 0.911151]
[Epoch 1/40] [Batch 62/196] [Batch classify loss: 0.815188]
[Epoch 1/40] [Batch 63/196] [Batch classify loss: 0.838032]
[Epoch 1/40] [Batch 64/196] [Batch classify loss: 0.849524]
[Epoch 1/40] [Batch 65/196] [Batch classify loss: 0.870126]
[Epoch 1/40] [Batch 66/196] [Batch classify loss: 0.894229]
[Epoch 1/40] [Batch 67/196] [Batch classify loss: 0.860978]
[Epoch 1/40] [Batch 68/196] [Batch classify loss: 0.882565]
[Epoch 1/40] [Batch 69/196] [Batch classify loss: 0.934281]
[Epoch 1/40] [Batch 70/196] [Batch classify loss: 0.904592]
[Epoch 1/40] [Batch 71/196] [Batch classify loss: 0.859049]
[Epoch 1/40] [Batch 72/196] [Batch classify loss: 0.851628]
[Epoch 1/40] [Batch 73/196] [Batch classify loss: 0.873998]
[Epoch 1/40] [Batch 74/196] [Batch classify loss: 0.876516]
[Epoch 1/40] [Batch 75/196] [Batch classify loss: 0.864719]
[Epoch 1/40] [Batch 76/196] [Batch classify loss: 0.793510]
[Epoch 1/40] [Batch 77/196] [Batch classify loss: 0.860333]
[Epoch 1/40] [Batch 78/196] [Batch classify loss: 0.936697]
[Epoch 1/40] [Batch 79/196] [Batch classify loss: 0.900010]
[Epoch 1/40] [Batch 80/196] [Batch classify loss: 0.843802]
[Epoch 1/40] [Batch 81/196] [Batch classify loss: 0.895840]
[Epoch 1/40] [Batch 82/196] [Batch classify loss: 0.879218]
[Epoch 1/40] [Batch 83/196] [Batch classify loss: 0.890497]
[Epoch 1/40] [Batch 84/196] [Batch classify loss: 0.887481]
[Epoch 1/40] [Batch 85/196] [Batch classify loss: 0.956441]
[Epoch 1/40] [Batch 86/196] [Batch classify loss: 0.839537]
[Epoch 1/40] [Batch 87/196] [Batch classify loss: 0.825772]
[Epoch 1/40] [Batch 88/196] [Batch classify loss: 0.915098]
[Epoch 1/40] [Batch 89/196] [Batch classify loss: 0.899025]
[Epoch 1/40] [Batch 90/196] [Batch classify loss: 0.876541]
[Epoch 1/40] [Batch 91/196] [Batch classify loss: 0.892077]
[Epoch 1/40] [Batch 92/196] [Batch classify loss: 0.844768]
[Epoch 1/40] [Batch 93/196] [Batch classify loss: 0.944939]
[Epoch 1/40] [Batch 94/196] [Batch classify loss: 0.804171]
[Epoch 1/40] [Batch 95/196] [Batch classify loss: 0.886011]
[Epoch 1/40] [Batch 96/196] [Batch classify loss: 0.934132]
[Epoch 1/40] [Batch 97/196] [Batch classify loss: 0.859441]
[Epoch 1/40] [Batch 98/196] [Batch classify loss: 0.818661]
[Epoch 1/40] [Batch 99/196] [Batch classify loss: 0.808034]
[Epoch 1/40] [Batch 100/196] [Batch classify loss: 0.466094]
[Epoch 1/40] [Batch 101/196] [Batch classify loss: 0.794777]
[Epoch 1/40] [Batch 102/196] [Batch classify loss: 0.834011]
[Epoch 1/40] [Batch 103/196] [Batch classify loss: 0.793387]
[Epoch 1/40] [Batch 104/196] [Batch classify loss: 0.890091]
[Epoch 1/40] [Batch 105/196] [Batch classify loss: 0.876655]
[Epoch 1/40] [Batch 106/196] [Batch classify loss: 0.733037]
[Epoch 1/40] [Batch 107/196] [Batch classify loss: 0.891008]
[Epoch 1/40] [Batch 108/196] [Batch classify loss: 0.833936]
[Epoch 1/40] [Batch 109/196] [Batch classify loss: 0.865345]
[Epoch 1/40] [Batch 110/196] [Batch classify loss: 0.895052]
[Epoch 1/40] [Batch 111/196] [Batch classify loss: 0.874837]
[Epoch 1/40] [Batch 112/196] [Batch classify loss: 0.848678]
[Epoch 1/40] [Batch 113/196] [Batch classify loss: 0.921978]
[Epoch 1/40] [Batch 114/196] [Batch classify loss: 0.836906]
[Epoch 1/40] [Batch 115/196] [Batch classify loss: 0.823650]
[Epoch 1/40] [Batch 116/196] [Batch classify loss: 0.830548]
[Epoch 1/40] [Batch 117/196] [Batch classify loss: 0.822128]
[Epoch 1/40] [Batch 118/196] [Batch classify loss: 0.795888]
[Epoch 1/40] [Batch 119/196] [Batch classify loss: 0.817667]
[Epoch 1/40] [Batch 120/196] [Batch classify loss: 0.752063]
[Epoch 1/40] [Batch 121/196] [Batch classify loss: 0.837980]
[Epoch 1/40] [Batch 122/196] [Batch classify loss: 0.819039]
[Epoch 1/40] [Batch 123/196] [Batch classify loss: 0.796852]
[Epoch 1/40] [Batch 124/196] [Batch classify loss: 0.857034]
[Epoch 1/40] [Batch 125/196] [Batch classify loss: 0.849290]
[Epoch 1/40] [Batch 126/196] [Batch classify loss: 0.771635]
[Epoch 1/40] [Batch 127/196] [Batch classify loss: 0.907284]
[Epoch 1/40] [Batch 128/196] [Batch classify loss: 0.847181]
[Epoch 1/40] [Batch 129/196] [Batch classify loss: 0.920410]
[Epoch 1/40] [Batch 130/196] [Batch classify loss: 0.898049]
[Epoch 1/40] [Batch 131/196] [Batch classify loss: 0.856922]
[Epoch 1/40] [Batch 132/196] [Batch classify loss: 0.835395]
[Epoch 1/40] [Batch 133/196] [Batch classify loss: 0.883323]
[Epoch 1/40] [Batch 134/196] [Batch classify loss: 0.846013]
[Epoch 1/40] [Batch 135/196] [Batch classify loss: 0.807803]
[Epoch 1/40] [Batch 136/196] [Batch classify loss: 0.860869]
[Epoch 1/40] [Batch 137/196] [Batch classify loss: 0.879814]
[Epoch 1/40] [Batch 138/196] [Batch classify loss: 0.833517]
[Epoch 1/40] [Batch 139/196] [Batch classify loss: 0.803261]
[Epoch 1/40] [Batch 140/196] [Batch classify loss: 0.860029]
[Epoch 1/40] [Batch 141/196] [Batch classify loss: 0.862033]
[Epoch 1/40] [Batch 142/196] [Batch classify loss: 0.921191]
[Epoch 1/40] [Batch 143/196] [Batch classify loss: 0.816114]
[Epoch 1/40] [Batch 144/196] [Batch classify loss: 0.879317]
[Epoch 1/40] [Batch 145/196] [Batch classify loss: 0.817321]
[Epoch 1/40] [Batch 146/196] [Batch classify loss: 0.818864]
[Epoch 1/40] [Batch 147/196] [Batch classify loss: 0.950006]
[Epoch 1/40] [Batch 148/196] [Batch classify loss: 0.862733]
[Epoch 1/40] [Batch 149/196] [Batch classify loss: 0.832611]
[Epoch 1/40] [Batch 150/196] [Batch classify loss: 0.950716]
[Epoch 1/40] [Batch 151/196] [Batch classify loss: 0.788652]
[Epoch 1/40] [Batch 152/196] [Batch classify loss: 0.814994]
[Epoch 1/40] [Batch 153/196] [Batch classify loss: 0.814607]
[Epoch 1/40] [Batch 154/196] [Batch classify loss: 0.835868]
[Epoch 1/40] [Batch 155/196] [Batch classify loss: 0.852035]
[Epoch 1/40] [Batch 156/196] [Batch classify loss: 0.866517]
[Epoch 1/40] [Batch 157/196] [Batch classify loss: 0.869911]
[Epoch 1/40] [Batch 158/196] [Batch classify loss: 0.821829]
[Epoch 1/40] [Batch 159/196] [Batch classify loss: 0.827619]
[Epoch 1/40] [Batch 160/196] [Batch classify loss: 0.825624]
[Epoch 1/40] [Batch 161/196] [Batch classify loss: 0.763360]
[Epoch 1/40] [Batch 162/196] [Batch classify loss: 0.897660]
[Epoch 1/40] [Batch 163/196] [Batch classify loss: 0.836768]
[Epoch 1/40] [Batch 164/196] [Batch classify loss: 0.863945]
[Epoch 1/40] [Batch 165/196] [Batch classify loss: 0.805846]
[Epoch 1/40] [Batch 166/196] [Batch classify loss: 0.808482]
[Epoch 1/40] [Batch 167/196] [Batch classify loss: 0.820025]
[Epoch 1/40] [Batch 168/196] [Batch classify loss: 0.923514]
[Epoch 1/40] [Batch 169/196] [Batch classify loss: 0.884061]
[Epoch 1/40] [Batch 170/196] [Batch classify loss: 0.786880]
[Epoch 1/40] [Batch 171/196] [Batch classify loss: 0.816103]
[Epoch 1/40] [Batch 172/196] [Batch classify loss: 0.858075]
[Epoch 1/40] [Batch 173/196] [Batch classify loss: 0.830938]
[Epoch 1/40] [Batch 174/196] [Batch classify loss: 0.853288]
[Epoch 1/40] [Batch 175/196] [Batch classify loss: 0.785176]
[Epoch 1/40] [Batch 176/196] [Batch classify loss: 0.824191]
[Epoch 1/40] [Batch 177/196] [Batch classify loss: 0.849972]
[Epoch 1/40] [Batch 178/196] [Batch classify loss: 0.842178]
[Epoch 1/40] [Batch 179/196] [Batch classify loss: 0.887904]
[Epoch 1/40] [Batch 180/196] [Batch classify loss: 0.836573]
[Epoch 1/40] [Batch 181/196] [Batch classify loss: 0.768656]
[Epoch 1/40] [Batch 182/196] [Batch classify loss: 0.813901]
[Epoch 1/40] [Batch 183/196] [Batch classify loss: 0.881716]
[Epoch 1/40] [Batch 184/196] [Batch classify loss: 0.797299]
[Epoch 1/40] [Batch 185/196] [Batch classify loss: 0.859577]
[Epoch 1/40] [Batch 186/196] [Batch classify loss: 0.816346]
[Epoch 1/40] [Batch 187/196] [Batch classify loss: 0.893825]
[Epoch 1/40] [Batch 188/196] [Batch classify loss: 0.904671]
[Epoch 1/40] [Batch 189/196] [Batch classify loss: 0.874265]
[Epoch 1/40] [Batch 190/196] [Batch classify loss: 0.922428]
[Epoch 1/40] [Batch 191/196] [Batch classify loss: 0.840585]
[Epoch 1/40] [Batch 192/196] [Batch classify loss: 0.855710]
[Epoch 1/40] [Batch 193/196] [Batch classify loss: 0.844159]
[Epoch 1/40] [Batch 194/196] [Batch classify loss: 0.851422]
[Epoch 1/40] [Batch 195/196] [Batch classify loss: 0.855538]
[Epoch 1/40] [Batch 196/196] [Batch classify loss: 1.278755]
0001 epoch rmt trained classifier accuary on the clean testing examples:77.6500%
0001 epoch rmt trained classifier loss on the clean testing examples:0.7055
initlize attack classifier
generate pixel adversarial exampels
Get FGSM examples generate model
self._args.attack_eps: 0.3
generating testset adversarial examples...
finished generate testset adversarial examples !
0001 epoch rmt trained classifier accuary on adversarial testset:8.8500%
0001 epoch rmt trained classifier loss on adversarial testset:4.026493549346924


1epoch learning rate:0.001
[Epoch 2/40] [Batch 1/196] [Batch classify loss: 0.742611]
[Epoch 2/40] [Batch 2/196] [Batch classify loss: 0.751518]
[Epoch 2/40] [Batch 3/196] [Batch classify loss: 0.738810]
[Epoch 2/40] [Batch 4/196] [Batch classify loss: 0.802200]
[Epoch 2/40] [Batch 5/196] [Batch classify loss: 0.726186]
[Epoch 2/40] [Batch 6/196] [Batch classify loss: 0.760513]
[Epoch 2/40] [Batch 7/196] [Batch classify loss: 0.802604]
[Epoch 2/40] [Batch 8/196] [Batch classify loss: 0.787964]
[Epoch 2/40] [Batch 9/196] [Batch classify loss: 0.781733]
[Epoch 2/40] [Batch 10/196] [Batch classify loss: 0.768611]
[Epoch 2/40] [Batch 11/196] [Batch classify loss: 0.713452]
[Epoch 2/40] [Batch 12/196] [Batch classify loss: 0.783046]
[Epoch 2/40] [Batch 13/196] [Batch classify loss: 0.753662]
[Epoch 2/40] [Batch 14/196] [Batch classify loss: 0.802397]
[Epoch 2/40] [Batch 15/196] [Batch classify loss: 0.753561]
[Epoch 2/40] [Batch 16/196] [Batch classify loss: 0.797257]
[Epoch 2/40] [Batch 17/196] [Batch classify loss: 0.761138]
[Epoch 2/40] [Batch 18/196] [Batch classify loss: 0.751387]
[Epoch 2/40] [Batch 19/196] [Batch classify loss: 0.807501]
[Epoch 2/40] [Batch 20/196] [Batch classify loss: 0.798821]
[Epoch 2/40] [Batch 21/196] [Batch classify loss: 0.769165]
[Epoch 2/40] [Batch 22/196] [Batch classify loss: 0.761934]
[Epoch 2/40] [Batch 23/196] [Batch classify loss: 0.779813]
[Epoch 2/40] [Batch 24/196] [Batch classify loss: 0.760575]
[Epoch 2/40] [Batch 25/196] [Batch classify loss: 0.756083]
[Epoch 2/40] [Batch 26/196] [Batch classify loss: 0.720867]
[Epoch 2/40] [Batch 27/196] [Batch classify loss: 0.824796]
[Epoch 2/40] [Batch 28/196] [Batch classify loss: 0.806534]
[Epoch 2/40] [Batch 29/196] [Batch classify loss: 0.761290]
[Epoch 2/40] [Batch 30/196] [Batch classify loss: 0.836410]
[Epoch 2/40] [Batch 31/196] [Batch classify loss: 0.796772]
[Epoch 2/40] [Batch 32/196] [Batch classify loss: 0.810981]
[Epoch 2/40] [Batch 33/196] [Batch classify loss: 0.806037]
[Epoch 2/40] [Batch 34/196] [Batch classify loss: 0.747135]
[Epoch 2/40] [Batch 35/196] [Batch classify loss: 0.780208]
[Epoch 2/40] [Batch 36/196] [Batch classify loss: 0.759257]
[Epoch 2/40] [Batch 37/196] [Batch classify loss: 0.819581]
[Epoch 2/40] [Batch 38/196] [Batch classify loss: 0.709600]
[Epoch 2/40] [Batch 39/196] [Batch classify loss: 0.865305]
[Epoch 2/40] [Batch 40/196] [Batch classify loss: 0.806652]
[Epoch 2/40] [Batch 41/196] [Batch classify loss: 0.731894]
[Epoch 2/40] [Batch 42/196] [Batch classify loss: 0.752279]
[Epoch 2/40] [Batch 43/196] [Batch classify loss: 0.848605]
[Epoch 2/40] [Batch 44/196] [Batch classify loss: 0.745161]
[Epoch 2/40] [Batch 45/196] [Batch classify loss: 0.788014]
[Epoch 2/40] [Batch 46/196] [Batch classify loss: 0.831430]
[Epoch 2/40] [Batch 47/196] [Batch classify loss: 0.739241]
[Epoch 2/40] [Batch 48/196] [Batch classify loss: 0.788355]
[Epoch 2/40] [Batch 49/196] [Batch classify loss: 0.800563]
[Epoch 2/40] [Batch 50/196] [Batch classify loss: 0.734563]
[Epoch 2/40] [Batch 51/196] [Batch classify loss: 0.865132]
[Epoch 2/40] [Batch 52/196] [Batch classify loss: 0.819478]
[Epoch 2/40] [Batch 53/196] [Batch classify loss: 0.752305]
[Epoch 2/40] [Batch 54/196] [Batch classify loss: 0.778220]
[Epoch 2/40] [Batch 55/196] [Batch classify loss: 0.791131]
[Epoch 2/40] [Batch 56/196] [Batch classify loss: 0.808808]
[Epoch 2/40] [Batch 57/196] [Batch classify loss: 0.765481]
[Epoch 2/40] [Batch 58/196] [Batch classify loss: 0.806141]
[Epoch 2/40] [Batch 59/196] [Batch classify loss: 0.772065]
[Epoch 2/40] [Batch 60/196] [Batch classify loss: 0.791919]
[Epoch 2/40] [Batch 61/196] [Batch classify loss: 0.797902]
[Epoch 2/40] [Batch 62/196] [Batch classify loss: 0.759010]
[Epoch 2/40] [Batch 63/196] [Batch classify loss: 0.673979]
[Epoch 2/40] [Batch 64/196] [Batch classify loss: 0.813798]
[Epoch 2/40] [Batch 65/196] [Batch classify loss: 0.745789]
[Epoch 2/40] [Batch 66/196] [Batch classify loss: 0.801810]
[Epoch 2/40] [Batch 67/196] [Batch classify loss: 0.757180]
[Epoch 2/40] [Batch 68/196] [Batch classify loss: 0.758568]
[Epoch 2/40] [Batch 69/196] [Batch classify loss: 0.765122]
[Epoch 2/40] [Batch 70/196] [Batch classify loss: 0.807326]
[Epoch 2/40] [Batch 71/196] [Batch classify loss: 0.841404]
[Epoch 2/40] [Batch 72/196] [Batch classify loss: 0.756884]
[Epoch 2/40] [Batch 73/196] [Batch classify loss: 0.799551]
[Epoch 2/40] [Batch 74/196] [Batch classify loss: 0.718297]
[Epoch 2/40] [Batch 75/196] [Batch classify loss: 0.757186]
[Epoch 2/40] [Batch 76/196] [Batch classify loss: 0.756921]
[Epoch 2/40] [Batch 77/196] [Batch classify loss: 0.795414]
[Epoch 2/40] [Batch 78/196] [Batch classify loss: 0.759462]
[Epoch 2/40] [Batch 79/196] [Batch classify loss: 0.745357]
[Epoch 2/40] [Batch 80/196] [Batch classify loss: 0.808616]
[Epoch 2/40] [Batch 81/196] [Batch classify loss: 0.771418]
[Epoch 2/40] [Batch 82/196] [Batch classify loss: 0.840198]
[Epoch 2/40] [Batch 83/196] [Batch classify loss: 0.797591]
[Epoch 2/40] [Batch 84/196] [Batch classify loss: 0.810605]
[Epoch 2/40] [Batch 85/196] [Batch classify loss: 0.716452]
[Epoch 2/40] [Batch 86/196] [Batch classify loss: 0.865727]
[Epoch 2/40] [Batch 87/196] [Batch classify loss: 0.779323]
[Epoch 2/40] [Batch 88/196] [Batch classify loss: 0.803319]
[Epoch 2/40] [Batch 89/196] [Batch classify loss: 0.754239]
[Epoch 2/40] [Batch 90/196] [Batch classify loss: 0.796610]
[Epoch 2/40] [Batch 91/196] [Batch classify loss: 0.770073]
[Epoch 2/40] [Batch 92/196] [Batch classify loss: 0.790603]
[Epoch 2/40] [Batch 93/196] [Batch classify loss: 0.762917]
[Epoch 2/40] [Batch 94/196] [Batch classify loss: 0.708465]
[Epoch 2/40] [Batch 95/196] [Batch classify loss: 0.776454]
[Epoch 2/40] [Batch 96/196] [Batch classify loss: 0.736441]
[Epoch 2/40] [Batch 97/196] [Batch classify loss: 0.741802]
[Epoch 2/40] [Batch 98/196] [Batch classify loss: 0.825063]
[Epoch 2/40] [Batch 99/196] [Batch classify loss: 0.780445]
[Epoch 2/40] [Batch 100/196] [Batch classify loss: 0.448173]
[Epoch 2/40] [Batch 101/196] [Batch classify loss: 0.613917]
[Epoch 2/40] [Batch 102/196] [Batch classify loss: 0.666017]
[Epoch 2/40] [Batch 103/196] [Batch classify loss: 0.751534]
[Epoch 2/40] [Batch 104/196] [Batch classify loss: 0.758254]
[Epoch 2/40] [Batch 105/196] [Batch classify loss: 0.731154]
[Epoch 2/40] [Batch 106/196] [Batch classify loss: 0.746600]
[Epoch 2/40] [Batch 107/196] [Batch classify loss: 0.685677]
[Epoch 2/40] [Batch 108/196] [Batch classify loss: 0.715655]
[Epoch 2/40] [Batch 109/196] [Batch classify loss: 0.687825]
[Epoch 2/40] [Batch 110/196] [Batch classify loss: 0.749891]
[Epoch 2/40] [Batch 111/196] [Batch classify loss: 0.692401]
[Epoch 2/40] [Batch 112/196] [Batch classify loss: 0.679704]
[Epoch 2/40] [Batch 113/196] [Batch classify loss: 0.730130]
[Epoch 2/40] [Batch 114/196] [Batch classify loss: 0.714939]
[Epoch 2/40] [Batch 115/196] [Batch classify loss: 0.713770]
[Epoch 2/40] [Batch 116/196] [Batch classify loss: 0.777550]
[Epoch 2/40] [Batch 117/196] [Batch classify loss: 0.728057]
[Epoch 2/40] [Batch 118/196] [Batch classify loss: 0.675708]
[Epoch 2/40] [Batch 119/196] [Batch classify loss: 0.749981]
[Epoch 2/40] [Batch 120/196] [Batch classify loss: 0.706430]
[Epoch 2/40] [Batch 121/196] [Batch classify loss: 0.717607]
[Epoch 2/40] [Batch 122/196] [Batch classify loss: 0.738398]
[Epoch 2/40] [Batch 123/196] [Batch classify loss: 0.651329]
[Epoch 2/40] [Batch 124/196] [Batch classify loss: 0.766233]
[Epoch 2/40] [Batch 125/196] [Batch classify loss: 0.746143]
[Epoch 2/40] [Batch 126/196] [Batch classify loss: 0.729894]
[Epoch 2/40] [Batch 127/196] [Batch classify loss: 0.716964]
[Epoch 2/40] [Batch 128/196] [Batch classify loss: 0.742355]
[Epoch 2/40] [Batch 129/196] [Batch classify loss: 0.733786]
[Epoch 2/40] [Batch 130/196] [Batch classify loss: 0.844509]
[Epoch 2/40] [Batch 131/196] [Batch classify loss: 0.791614]
[Epoch 2/40] [Batch 132/196] [Batch classify loss: 0.701101]
[Epoch 2/40] [Batch 133/196] [Batch classify loss: 0.725909]
[Epoch 2/40] [Batch 134/196] [Batch classify loss: 0.734150]
[Epoch 2/40] [Batch 135/196] [Batch classify loss: 0.790876]
[Epoch 2/40] [Batch 136/196] [Batch classify loss: 0.782359]
[Epoch 2/40] [Batch 137/196] [Batch classify loss: 0.696208]
[Epoch 2/40] [Batch 138/196] [Batch classify loss: 0.679512]
[Epoch 2/40] [Batch 139/196] [Batch classify loss: 0.763762]
[Epoch 2/40] [Batch 140/196] [Batch classify loss: 0.745839]
[Epoch 2/40] [Batch 141/196] [Batch classify loss: 0.791404]
[Epoch 2/40] [Batch 142/196] [Batch classify loss: 0.761773]
[Epoch 2/40] [Batch 143/196] [Batch classify loss: 0.848054]
[Epoch 2/40] [Batch 144/196] [Batch classify loss: 0.795554]
[Epoch 2/40] [Batch 145/196] [Batch classify loss: 0.735782]
[Epoch 2/40] [Batch 146/196] [Batch classify loss: 0.722728]
[Epoch 2/40] [Batch 147/196] [Batch classify loss: 0.762103]
[Epoch 2/40] [Batch 148/196] [Batch classify loss: 0.745781]
[Epoch 2/40] [Batch 149/196] [Batch classify loss: 0.818418]
[Epoch 2/40] [Batch 150/196] [Batch classify loss: 0.807105]
[Epoch 2/40] [Batch 151/196] [Batch classify loss: 0.689075]
[Epoch 2/40] [Batch 152/196] [Batch classify loss: 0.768324]
[Epoch 2/40] [Batch 153/196] [Batch classify loss: 0.725428]
[Epoch 2/40] [Batch 154/196] [Batch classify loss: 0.770066]
[Epoch 2/40] [Batch 155/196] [Batch classify loss: 0.778491]
[Epoch 2/40] [Batch 156/196] [Batch classify loss: 0.797519]
[Epoch 2/40] [Batch 157/196] [Batch classify loss: 0.722374]
[Epoch 2/40] [Batch 158/196] [Batch classify loss: 0.794444]
[Epoch 2/40] [Batch 159/196] [Batch classify loss: 0.715524]
[Epoch 2/40] [Batch 160/196] [Batch classify loss: 0.675261]
[Epoch 2/40] [Batch 161/196] [Batch classify loss: 0.793858]
[Epoch 2/40] [Batch 162/196] [Batch classify loss: 0.734697]
[Epoch 2/40] [Batch 163/196] [Batch classify loss: 0.766726]
[Epoch 2/40] [Batch 164/196] [Batch classify loss: 0.775533]
[Epoch 2/40] [Batch 165/196] [Batch classify loss: 0.725143]
[Epoch 2/40] [Batch 166/196] [Batch classify loss: 0.770453]
[Epoch 2/40] [Batch 167/196] [Batch classify loss: 0.714343]
[Epoch 2/40] [Batch 168/196] [Batch classify loss: 0.718499]
[Epoch 2/40] [Batch 169/196] [Batch classify loss: 0.726630]
[Epoch 2/40] [Batch 170/196] [Batch classify loss: 0.712853]
[Epoch 2/40] [Batch 171/196] [Batch classify loss: 0.729536]
[Epoch 2/40] [Batch 172/196] [Batch classify loss: 0.723418]
[Epoch 2/40] [Batch 173/196] [Batch classify loss: 0.740738]
[Epoch 2/40] [Batch 174/196] [Batch classify loss: 0.745600]
[Epoch 2/40] [Batch 175/196] [Batch classify loss: 0.765961]
[Epoch 2/40] [Batch 176/196] [Batch classify loss: 0.698887]
[Epoch 2/40] [Batch 177/196] [Batch classify loss: 0.744351]
[Epoch 2/40] [Batch 178/196] [Batch classify loss: 0.789504]
[Epoch 2/40] [Batch 179/196] [Batch classify loss: 0.747669]
[Epoch 2/40] [Batch 180/196] [Batch classify loss: 0.816069]
[Epoch 2/40] [Batch 181/196] [Batch classify loss: 0.733661]
[Epoch 2/40] [Batch 182/196] [Batch classify loss: 0.738416]
[Epoch 2/40] [Batch 183/196] [Batch classify loss: 0.742025]
[Epoch 2/40] [Batch 184/196] [Batch classify loss: 0.708735]
[Epoch 2/40] [Batch 185/196] [Batch classify loss: 0.703848]
[Epoch 2/40] [Batch 186/196] [Batch classify loss: 0.811314]
[Epoch 2/40] [Batch 187/196] [Batch classify loss: 0.724887]
[Epoch 2/40] [Batch 188/196] [Batch classify loss: 0.728745]
[Epoch 2/40] [Batch 189/196] [Batch classify loss: 0.776804]
[Epoch 2/40] [Batch 190/196] [Batch classify loss: 0.844057]
[Epoch 2/40] [Batch 191/196] [Batch classify loss: 0.781820]
[Epoch 2/40] [Batch 192/196] [Batch classify loss: 0.772356]
[Epoch 2/40] [Batch 193/196] [Batch classify loss: 0.785117]
[Epoch 2/40] [Batch 194/196] [Batch classify loss: 0.741212]
[Epoch 2/40] [Batch 195/196] [Batch classify loss: 0.861792]
[Epoch 2/40] [Batch 196/196] [Batch classify loss: 1.093789]
0002 epoch rmt trained classifier accuary on the clean testing examples:82.9100%
0002 epoch rmt trained classifier loss on the clean testing examples:0.5390
initlize attack classifier
generate pixel adversarial exampels
Get FGSM examples generate model
self._args.attack_eps: 0.3
generating testset adversarial examples...
finished generate testset adversarial examples !
0002 epoch rmt trained classifier accuary on adversarial testset:14.8400%
0002 epoch rmt trained classifier loss on adversarial testset:4.046351432800293


2epoch learning rate:0.001
[Epoch 3/40] [Batch 1/196] [Batch classify loss: 0.726751]
[Epoch 3/40] [Batch 2/196] [Batch classify loss: 0.691157]
[Epoch 3/40] [Batch 3/196] [Batch classify loss: 0.626193]
[Epoch 3/40] [Batch 4/196] [Batch classify loss: 0.652691]
[Epoch 3/40] [Batch 5/196] [Batch classify loss: 0.666313]
[Epoch 3/40] [Batch 6/196] [Batch classify loss: 0.634772]
[Epoch 3/40] [Batch 7/196] [Batch classify loss: 0.629469]
[Epoch 3/40] [Batch 8/196] [Batch classify loss: 0.685200]
[Epoch 3/40] [Batch 9/196] [Batch classify loss: 0.616770]
[Epoch 3/40] [Batch 10/196] [Batch classify loss: 0.672038]
[Epoch 3/40] [Batch 11/196] [Batch classify loss: 0.646445]
[Epoch 3/40] [Batch 12/196] [Batch classify loss: 0.628612]
[Epoch 3/40] [Batch 13/196] [Batch classify loss: 0.685933]
[Epoch 3/40] [Batch 14/196] [Batch classify loss: 0.709378]
[Epoch 3/40] [Batch 15/196] [Batch classify loss: 0.643530]
[Epoch 3/40] [Batch 16/196] [Batch classify loss: 0.645472]
[Epoch 3/40] [Batch 17/196] [Batch classify loss: 0.663897]
[Epoch 3/40] [Batch 18/196] [Batch classify loss: 0.670347]
[Epoch 3/40] [Batch 19/196] [Batch classify loss: 0.641929]
[Epoch 3/40] [Batch 20/196] [Batch classify loss: 0.670135]
[Epoch 3/40] [Batch 21/196] [Batch classify loss: 0.597621]
[Epoch 3/40] [Batch 22/196] [Batch classify loss: 0.672302]
[Epoch 3/40] [Batch 23/196] [Batch classify loss: 0.680450]
[Epoch 3/40] [Batch 24/196] [Batch classify loss: 0.653226]
[Epoch 3/40] [Batch 25/196] [Batch classify loss: 0.718599]
[Epoch 3/40] [Batch 26/196] [Batch classify loss: 0.702737]
[Epoch 3/40] [Batch 27/196] [Batch classify loss: 0.644558]
[Epoch 3/40] [Batch 28/196] [Batch classify loss: 0.686198]
[Epoch 3/40] [Batch 29/196] [Batch classify loss: 0.636083]
[Epoch 3/40] [Batch 30/196] [Batch classify loss: 0.637647]
[Epoch 3/40] [Batch 31/196] [Batch classify loss: 0.677976]
[Epoch 3/40] [Batch 32/196] [Batch classify loss: 0.633339]
[Epoch 3/40] [Batch 33/196] [Batch classify loss: 0.673472]
[Epoch 3/40] [Batch 34/196] [Batch classify loss: 0.636808]
[Epoch 3/40] [Batch 35/196] [Batch classify loss: 0.602745]
[Epoch 3/40] [Batch 36/196] [Batch classify loss: 0.619318]
[Epoch 3/40] [Batch 37/196] [Batch classify loss: 0.702670]
[Epoch 3/40] [Batch 38/196] [Batch classify loss: 0.667878]
[Epoch 3/40] [Batch 39/196] [Batch classify loss: 0.657601]
[Epoch 3/40] [Batch 40/196] [Batch classify loss: 0.633969]
[Epoch 3/40] [Batch 41/196] [Batch classify loss: 0.686611]
[Epoch 3/40] [Batch 42/196] [Batch classify loss: 0.651971]
[Epoch 3/40] [Batch 43/196] [Batch classify loss: 0.634658]
[Epoch 3/40] [Batch 44/196] [Batch classify loss: 0.646602]
[Epoch 3/40] [Batch 45/196] [Batch classify loss: 0.702168]
[Epoch 3/40] [Batch 46/196] [Batch classify loss: 0.661254]
[Epoch 3/40] [Batch 47/196] [Batch classify loss: 0.660018]
[Epoch 3/40] [Batch 48/196] [Batch classify loss: 0.632537]
[Epoch 3/40] [Batch 49/196] [Batch classify loss: 0.671936]
[Epoch 3/40] [Batch 50/196] [Batch classify loss: 0.621688]
[Epoch 3/40] [Batch 51/196] [Batch classify loss: 0.728414]
[Epoch 3/40] [Batch 52/196] [Batch classify loss: 0.652136]
[Epoch 3/40] [Batch 53/196] [Batch classify loss: 0.673868]
[Epoch 3/40] [Batch 54/196] [Batch classify loss: 0.651906]
[Epoch 3/40] [Batch 55/196] [Batch classify loss: 0.655077]
[Epoch 3/40] [Batch 56/196] [Batch classify loss: 0.701911]
[Epoch 3/40] [Batch 57/196] [Batch classify loss: 0.714405]
[Epoch 3/40] [Batch 58/196] [Batch classify loss: 0.638316]
[Epoch 3/40] [Batch 59/196] [Batch classify loss: 0.687020]
[Epoch 3/40] [Batch 60/196] [Batch classify loss: 0.681415]
[Epoch 3/40] [Batch 61/196] [Batch classify loss: 0.632270]
[Epoch 3/40] [Batch 62/196] [Batch classify loss: 0.645445]
[Epoch 3/40] [Batch 63/196] [Batch classify loss: 0.694059]
[Epoch 3/40] [Batch 64/196] [Batch classify loss: 0.687506]
[Epoch 3/40] [Batch 65/196] [Batch classify loss: 0.654586]
[Epoch 3/40] [Batch 66/196] [Batch classify loss: 0.645187]
[Epoch 3/40] [Batch 67/196] [Batch classify loss: 0.691327]
[Epoch 3/40] [Batch 68/196] [Batch classify loss: 0.683374]
[Epoch 3/40] [Batch 69/196] [Batch classify loss: 0.769247]
[Epoch 3/40] [Batch 70/196] [Batch classify loss: 0.677767]
[Epoch 3/40] [Batch 71/196] [Batch classify loss: 0.658401]
[Epoch 3/40] [Batch 72/196] [Batch classify loss: 0.665328]
[Epoch 3/40] [Batch 73/196] [Batch classify loss: 0.737650]
[Epoch 3/40] [Batch 74/196] [Batch classify loss: 0.718863]
[Epoch 3/40] [Batch 75/196] [Batch classify loss: 0.643425]
[Epoch 3/40] [Batch 76/196] [Batch classify loss: 0.636382]
[Epoch 3/40] [Batch 77/196] [Batch classify loss: 0.638373]
[Epoch 3/40] [Batch 78/196] [Batch classify loss: 0.723028]
[Epoch 3/40] [Batch 79/196] [Batch classify loss: 0.690552]
[Epoch 3/40] [Batch 80/196] [Batch classify loss: 0.681322]
[Epoch 3/40] [Batch 81/196] [Batch classify loss: 0.658166]
[Epoch 3/40] [Batch 82/196] [Batch classify loss: 0.693246]
[Epoch 3/40] [Batch 83/196] [Batch classify loss: 0.634062]
[Epoch 3/40] [Batch 84/196] [Batch classify loss: 0.687427]
[Epoch 3/40] [Batch 85/196] [Batch classify loss: 0.696780]
[Epoch 3/40] [Batch 86/196] [Batch classify loss: 0.592306]
[Epoch 3/40] [Batch 87/196] [Batch classify loss: 0.651843]
[Epoch 3/40] [Batch 88/196] [Batch classify loss: 0.718282]
[Epoch 3/40] [Batch 89/196] [Batch classify loss: 0.696541]
[Epoch 3/40] [Batch 90/196] [Batch classify loss: 0.675234]
[Epoch 3/40] [Batch 91/196] [Batch classify loss: 0.646384]
[Epoch 3/40] [Batch 92/196] [Batch classify loss: 0.663933]
[Epoch 3/40] [Batch 93/196] [Batch classify loss: 0.685041]
[Epoch 3/40] [Batch 94/196] [Batch classify loss: 0.721446]
[Epoch 3/40] [Batch 95/196] [Batch classify loss: 0.685095]
[Epoch 3/40] [Batch 96/196] [Batch classify loss: 0.692479]
[Epoch 3/40] [Batch 97/196] [Batch classify loss: 0.684568]
[Epoch 3/40] [Batch 98/196] [Batch classify loss: 0.698814]
[Epoch 3/40] [Batch 99/196] [Batch classify loss: 0.680230]
[Epoch 3/40] [Batch 100/196] [Batch classify loss: 0.419968]
[Epoch 3/40] [Batch 101/196] [Batch classify loss: 0.573339]
[Epoch 3/40] [Batch 102/196] [Batch classify loss: 0.542585]
[Epoch 3/40] [Batch 103/196] [Batch classify loss: 0.607726]
[Epoch 3/40] [Batch 104/196] [Batch classify loss: 0.542068]
[Epoch 3/40] [Batch 105/196] [Batch classify loss: 0.597513]
[Epoch 3/40] [Batch 106/196] [Batch classify loss: 0.618402]
[Epoch 3/40] [Batch 107/196] [Batch classify loss: 0.622895]
[Epoch 3/40] [Batch 108/196] [Batch classify loss: 0.651728]
[Epoch 3/40] [Batch 109/196] [Batch classify loss: 0.575957]
[Epoch 3/40] [Batch 110/196] [Batch classify loss: 0.547618]
[Epoch 3/40] [Batch 111/196] [Batch classify loss: 0.601975]
[Epoch 3/40] [Batch 112/196] [Batch classify loss: 0.630173]
[Epoch 3/40] [Batch 113/196] [Batch classify loss: 0.659147]
[Epoch 3/40] [Batch 114/196] [Batch classify loss: 0.648500]
[Epoch 3/40] [Batch 115/196] [Batch classify loss: 0.587378]
[Epoch 3/40] [Batch 116/196] [Batch classify loss: 0.663216]
[Epoch 3/40] [Batch 117/196] [Batch classify loss: 0.619687]
[Epoch 3/40] [Batch 118/196] [Batch classify loss: 0.661962]
[Epoch 3/40] [Batch 119/196] [Batch classify loss: 0.651593]
[Epoch 3/40] [Batch 120/196] [Batch classify loss: 0.615860]
[Epoch 3/40] [Batch 121/196] [Batch classify loss: 0.622971]
[Epoch 3/40] [Batch 122/196] [Batch classify loss: 0.637500]
[Epoch 3/40] [Batch 123/196] [Batch classify loss: 0.694894]
[Epoch 3/40] [Batch 124/196] [Batch classify loss: 0.647058]
[Epoch 3/40] [Batch 125/196] [Batch classify loss: 0.576381]
[Epoch 3/40] [Batch 126/196] [Batch classify loss: 0.594745]
[Epoch 3/40] [Batch 127/196] [Batch classify loss: 0.665384]
[Epoch 3/40] [Batch 128/196] [Batch classify loss: 0.597344]
[Epoch 3/40] [Batch 129/196] [Batch classify loss: 0.677930]
[Epoch 3/40] [Batch 130/196] [Batch classify loss: 0.666171]
[Epoch 3/40] [Batch 131/196] [Batch classify loss: 0.669039]
[Epoch 3/40] [Batch 132/196] [Batch classify loss: 0.659844]
[Epoch 3/40] [Batch 133/196] [Batch classify loss: 0.634638]
[Epoch 3/40] [Batch 134/196] [Batch classify loss: 0.575749]
[Epoch 3/40] [Batch 135/196] [Batch classify loss: 0.618696]
[Epoch 3/40] [Batch 136/196] [Batch classify loss: 0.635753]
[Epoch 3/40] [Batch 137/196] [Batch classify loss: 0.677615]
[Epoch 3/40] [Batch 138/196] [Batch classify loss: 0.596215]
[Epoch 3/40] [Batch 139/196] [Batch classify loss: 0.595900]
[Epoch 3/40] [Batch 140/196] [Batch classify loss: 0.674609]
[Epoch 3/40] [Batch 141/196] [Batch classify loss: 0.645804]
[Epoch 3/40] [Batch 142/196] [Batch classify loss: 0.688640]
[Epoch 3/40] [Batch 143/196] [Batch classify loss: 0.619981]
[Epoch 3/40] [Batch 144/196] [Batch classify loss: 0.659538]
[Epoch 3/40] [Batch 145/196] [Batch classify loss: 0.709035]
[Epoch 3/40] [Batch 146/196] [Batch classify loss: 0.655088]
[Epoch 3/40] [Batch 147/196] [Batch classify loss: 0.696188]
[Epoch 3/40] [Batch 148/196] [Batch classify loss: 0.678270]
[Epoch 3/40] [Batch 149/196] [Batch classify loss: 0.666600]
[Epoch 3/40] [Batch 150/196] [Batch classify loss: 0.651116]
[Epoch 3/40] [Batch 151/196] [Batch classify loss: 0.667611]
[Epoch 3/40] [Batch 152/196] [Batch classify loss: 0.651412]
[Epoch 3/40] [Batch 153/196] [Batch classify loss: 0.674072]
[Epoch 3/40] [Batch 154/196] [Batch classify loss: 0.652997]
[Epoch 3/40] [Batch 155/196] [Batch classify loss: 0.705942]
[Epoch 3/40] [Batch 156/196] [Batch classify loss: 0.640509]
[Epoch 3/40] [Batch 157/196] [Batch classify loss: 0.642811]
[Epoch 3/40] [Batch 158/196] [Batch classify loss: 0.654396]
[Epoch 3/40] [Batch 159/196] [Batch classify loss: 0.689671]
[Epoch 3/40] [Batch 160/196] [Batch classify loss: 0.664142]
[Epoch 3/40] [Batch 161/196] [Batch classify loss: 0.705234]
[Epoch 3/40] [Batch 162/196] [Batch classify loss: 0.663485]
[Epoch 3/40] [Batch 163/196] [Batch classify loss: 0.629762]
[Epoch 3/40] [Batch 164/196] [Batch classify loss: 0.687665]
[Epoch 3/40] [Batch 165/196] [Batch classify loss: 0.647666]
[Epoch 3/40] [Batch 166/196] [Batch classify loss: 0.614730]
[Epoch 3/40] [Batch 167/196] [Batch classify loss: 0.602137]
[Epoch 3/40] [Batch 168/196] [Batch classify loss: 0.680164]
[Epoch 3/40] [Batch 169/196] [Batch classify loss: 0.654917]
[Epoch 3/40] [Batch 170/196] [Batch classify loss: 0.675413]
[Epoch 3/40] [Batch 171/196] [Batch classify loss: 0.689272]
[Epoch 3/40] [Batch 172/196] [Batch classify loss: 0.665830]
[Epoch 3/40] [Batch 173/196] [Batch classify loss: 0.677818]
[Epoch 3/40] [Batch 174/196] [Batch classify loss: 0.701364]
[Epoch 3/40] [Batch 175/196] [Batch classify loss: 0.664179]
[Epoch 3/40] [Batch 176/196] [Batch classify loss: 0.640310]
[Epoch 3/40] [Batch 177/196] [Batch classify loss: 0.688985]
[Epoch 3/40] [Batch 178/196] [Batch classify loss: 0.697661]
[Epoch 3/40] [Batch 179/196] [Batch classify loss: 0.723426]
[Epoch 3/40] [Batch 180/196] [Batch classify loss: 0.684901]
[Epoch 3/40] [Batch 181/196] [Batch classify loss: 0.696395]
[Epoch 3/40] [Batch 182/196] [Batch classify loss: 0.703967]
[Epoch 3/40] [Batch 183/196] [Batch classify loss: 0.673882]
[Epoch 3/40] [Batch 184/196] [Batch classify loss: 0.715121]
[Epoch 3/40] [Batch 185/196] [Batch classify loss: 0.709398]
[Epoch 3/40] [Batch 186/196] [Batch classify loss: 0.722327]
[Epoch 3/40] [Batch 187/196] [Batch classify loss: 0.698786]
[Epoch 3/40] [Batch 188/196] [Batch classify loss: 0.709916]
[Epoch 3/40] [Batch 189/196] [Batch classify loss: 0.709728]
[Epoch 3/40] [Batch 190/196] [Batch classify loss: 0.670754]
[Epoch 3/40] [Batch 191/196] [Batch classify loss: 0.666563]
[Epoch 3/40] [Batch 192/196] [Batch classify loss: 0.670883]
[Epoch 3/40] [Batch 193/196] [Batch classify loss: 0.580307]
[Epoch 3/40] [Batch 194/196] [Batch classify loss: 0.646280]
[Epoch 3/40] [Batch 195/196] [Batch classify loss: 0.708590]
[Epoch 3/40] [Batch 196/196] [Batch classify loss: 0.868507]
0003 epoch rmt trained classifier accuary on the clean testing examples:77.4000%
0003 epoch rmt trained classifier loss on the clean testing examples:0.7611
initlize attack classifier
generate pixel adversarial exampels
Get FGSM examples generate model
self._args.attack_eps: 0.3
generating testset adversarial examples...
finished generate testset adversarial examples !
0003 epoch rmt trained classifier accuary on adversarial testset:10.0100%
0003 epoch rmt trained classifier loss on adversarial testset:4.246758937835693


3epoch learning rate:0.001
[Epoch 4/40] [Batch 1/196] [Batch classify loss: 0.471655]
[Epoch 4/40] [Batch 2/196] [Batch classify loss: 0.549595]
[Epoch 4/40] [Batch 3/196] [Batch classify loss: 0.564190]
[Epoch 4/40] [Batch 4/196] [Batch classify loss: 0.483455]
[Epoch 4/40] [Batch 5/196] [Batch classify loss: 0.563943]
[Epoch 4/40] [Batch 6/196] [Batch classify loss: 0.602540]
[Epoch 4/40] [Batch 7/196] [Batch classify loss: 0.557630]
[Epoch 4/40] [Batch 8/196] [Batch classify loss: 0.579810]
[Epoch 4/40] [Batch 9/196] [Batch classify loss: 0.524874]
[Epoch 4/40] [Batch 10/196] [Batch classify loss: 0.479513]
[Epoch 4/40] [Batch 11/196] [Batch classify loss: 0.588032]
[Epoch 4/40] [Batch 12/196] [Batch classify loss: 0.524945]
[Epoch 4/40] [Batch 13/196] [Batch classify loss: 0.545193]
[Epoch 4/40] [Batch 14/196] [Batch classify loss: 0.536121]
[Epoch 4/40] [Batch 15/196] [Batch classify loss: 0.605236]
[Epoch 4/40] [Batch 16/196] [Batch classify loss: 0.544485]
[Epoch 4/40] [Batch 17/196] [Batch classify loss: 0.563530]
