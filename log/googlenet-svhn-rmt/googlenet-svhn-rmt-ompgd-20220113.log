

---------------------------------------
Torch cuda is available
args.subcommand=run, run the command line
date: 20220114
blackbox attack
Experiment result save dir: /home/maggie/mmat/result/defense/rmt/om-pgd/basemixup-betasampler/googlenet-svhn/blackbox/20220114/00000
initilize the dataset loading parameters
Using downloaded and verified file: /home/data/maggie/svhn/train_32x32.mat
Loading *svhn* train dataloader finished !
Loading *svhn* test dataloader finished !
initlize classifier
learned calssify model != None
args.adv_dataset： /home/maggie/mmat/result/attack/pgd/googlenet-svhn/20220114/00000-ompgd-eps-0.02-acc-63.1700/attack-svhn-dataset/latent-attack-samples
Accuary of before rmt trained classifier on clean testset:93.8691%
Loss of before mmat trained classifier clean testset:0.24139124155044556
args.mix_mode: basemixup
args.mix_w_num: 2
args.beta_alpha: 2.0
args.dirichlet_gama: 1
cle_w_train.shape: torch.Size([24004, 8, 512])
cle_y_train.shape: torch.Size([24004, 8])
cle_x_test.shape: torch.Size([26032, 3, 32, 32])
cle_y_test.shape: torch.Size([26032])
adv_x_test.shape: torch.Size([10000, 3, 32, 32])
adv_y_test.shape: torch.Size([10000])
cle_train_dataloader.len: 287
Accuary of before rmt trained classifier on adversarial testset:63.1700%
Loss of before mmat trained classifier on adversarial testset:2.1092543601989746
w_trainset_len: 24004
batch_size: 256
w_batch_num: 94


0epoch learning rate:0.001
Setting up PyTorch plugin "bias_act_plugin"... /home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/utils/cpp_extension.py:286: UserWarning: 

                               !! WARNING !!

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Your compiler (c++) is not compatible with the compiler Pytorch was
built with for this platform, which is g++ on linux. Please
use g++ to to compile your extension. Alternatively, you may
compile PyTorch from source using c++, and then you can also use
c++ to compile your extension.

See https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md for help
with compiling PyTorch from source.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

                              !! WARNING !!

  platform=sys.platform))
Done.
Setting up PyTorch plugin "upfirdn2d_plugin"... /home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/utils/cpp_extension.py:286: UserWarning: 

                               !! WARNING !!

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Your compiler (c++) is not compatible with the compiler Pytorch was
built with for this platform, which is g++ on linux. Please
use g++ to to compile your extension. Alternatively, you may
compile PyTorch from source using c++, and then you can also use
c++ to compile your extension.

See https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md for help
with compiling PyTorch from source.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

                              !! WARNING !!

  platform=sys.platform))
Done.
[Epoch 1/40] [Batch 1/287] [Batch classify loss: 2.449460]
[Epoch 1/40] [Batch 2/287] [Batch classify loss: 1.834419]
[Epoch 1/40] [Batch 3/287] [Batch classify loss: 1.752899]
[Epoch 1/40] [Batch 4/287] [Batch classify loss: 1.654860]
[Epoch 1/40] [Batch 5/287] [Batch classify loss: 1.547677]
[Epoch 1/40] [Batch 6/287] [Batch classify loss: 1.460295]
[Epoch 1/40] [Batch 7/287] [Batch classify loss: 1.347327]
[Epoch 1/40] [Batch 8/287] [Batch classify loss: 1.394635]
[Epoch 1/40] [Batch 9/287] [Batch classify loss: 1.320708]
[Epoch 1/40] [Batch 10/287] [Batch classify loss: 1.332861]
[Epoch 1/40] [Batch 11/287] [Batch classify loss: 1.254758]
[Epoch 1/40] [Batch 12/287] [Batch classify loss: 1.286975]
[Epoch 1/40] [Batch 13/287] [Batch classify loss: 1.197989]
[Epoch 1/40] [Batch 14/287] [Batch classify loss: 1.256566]
[Epoch 1/40] [Batch 15/287] [Batch classify loss: 1.254365]
[Epoch 1/40] [Batch 16/287] [Batch classify loss: 1.289539]
[Epoch 1/40] [Batch 17/287] [Batch classify loss: 1.196345]
[Epoch 1/40] [Batch 18/287] [Batch classify loss: 1.258761]
[Epoch 1/40] [Batch 19/287] [Batch classify loss: 1.207945]
[Epoch 1/40] [Batch 20/287] [Batch classify loss: 1.199633]
[Epoch 1/40] [Batch 21/287] [Batch classify loss: 1.157198]
[Epoch 1/40] [Batch 22/287] [Batch classify loss: 1.151974]
[Epoch 1/40] [Batch 23/287] [Batch classify loss: 1.210403]
[Epoch 1/40] [Batch 24/287] [Batch classify loss: 1.211523]
[Epoch 1/40] [Batch 25/287] [Batch classify loss: 1.225168]
[Epoch 1/40] [Batch 26/287] [Batch classify loss: 1.219971]
[Epoch 1/40] [Batch 27/287] [Batch classify loss: 1.193811]
[Epoch 1/40] [Batch 28/287] [Batch classify loss: 1.225599]
[Epoch 1/40] [Batch 29/287] [Batch classify loss: 1.153737]
[Epoch 1/40] [Batch 30/287] [Batch classify loss: 1.185414]
[Epoch 1/40] [Batch 31/287] [Batch classify loss: 1.143048]
[Epoch 1/40] [Batch 32/287] [Batch classify loss: 1.207341]
[Epoch 1/40] [Batch 33/287] [Batch classify loss: 1.165370]
[Epoch 1/40] [Batch 34/287] [Batch classify loss: 1.125261]
[Epoch 1/40] [Batch 35/287] [Batch classify loss: 1.099673]
[Epoch 1/40] [Batch 36/287] [Batch classify loss: 1.121137]
[Epoch 1/40] [Batch 37/287] [Batch classify loss: 1.143126]
[Epoch 1/40] [Batch 38/287] [Batch classify loss: 1.120900]
[Epoch 1/40] [Batch 39/287] [Batch classify loss: 1.215905]


---------------------------------------
Torch cuda is available
args.subcommand=run, run the command line
date: 20220114
blackbox attack
Experiment result save dir: /home/maggie/mmat/result/defense/rmt/om-pgd/basemixup-betasampler/googlenet-svhn/blackbox/20220114/00001
initilize the dataset loading parameters
Using downloaded and verified file: /home/data/maggie/svhn/train_32x32.mat
[Epoch 1/40] [Batch 40/287] [Batch classify loss: 1.213537]
Loading *svhn* train dataloader finished !
Loading *svhn* test dataloader finished !
[Epoch 1/40] [Batch 41/287] [Batch classify loss: 1.124014]
[Epoch 1/40] [Batch 42/287] [Batch classify loss: 1.178600]
initlize classifier
learned calssify model != None
[Epoch 1/40] [Batch 43/287] [Batch classify loss: 1.134643]
[Epoch 1/40] [Batch 44/287] [Batch classify loss: 1.156134]
[Epoch 1/40] [Batch 45/287] [Batch classify loss: 1.222934]
[Epoch 1/40] [Batch 46/287] [Batch classify loss: 1.211257]
[Epoch 1/40] [Batch 47/287] [Batch classify loss: 1.111118]
[Epoch 1/40] [Batch 48/287] [Batch classify loss: 1.181151]
[Epoch 1/40] [Batch 49/287] [Batch classify loss: 1.147935]
[Epoch 1/40] [Batch 50/287] [Batch classify loss: 1.123448]
[Epoch 1/40] [Batch 51/287] [Batch classify loss: 1.230584]
[Epoch 1/40] [Batch 52/287] [Batch classify loss: 1.160238]
[Epoch 1/40] [Batch 53/287] [Batch classify loss: 1.139606]
[Epoch 1/40] [Batch 54/287] [Batch classify loss: 1.118232]
[Epoch 1/40] [Batch 55/287] [Batch classify loss: 1.178970]
[Epoch 1/40] [Batch 56/287] [Batch classify loss: 1.173905]
[Epoch 1/40] [Batch 57/287] [Batch classify loss: 1.224866]
[Epoch 1/40] [Batch 58/287] [Batch classify loss: 1.164153]
[Epoch 1/40] [Batch 59/287] [Batch classify loss: 1.121139]
[Epoch 1/40] [Batch 60/287] [Batch classify loss: 1.139234]
[Epoch 1/40] [Batch 61/287] [Batch classify loss: 1.131599]
args.adv_dataset： /home/maggie/mmat/result/attack/pgd/googlenet-svhn/20220112/00001-ompgd-eps-0.3-acc-0.6200/attack-svhn-dataset/latent-attack-samples
[Epoch 1/40] [Batch 62/287] [Batch classify loss: 1.195373]
[Epoch 1/40] [Batch 63/287] [Batch classify loss: 1.164299]
[Epoch 1/40] [Batch 64/287] [Batch classify loss: 1.131140]
[Epoch 1/40] [Batch 65/287] [Batch classify loss: 1.140257]
Accuary of before rmt trained classifier on clean testset:93.8691%
Loss of before mmat trained classifier clean testset:0.24139124155044556
args.mix_mode: basemixup
args.mix_w_num: 2
args.beta_alpha: 2.0
args.dirichlet_gama: 1
cle_w_train.shape: torch.Size([24004, 8, 512])
cle_y_train.shape: torch.Size([24004, 8])
cle_x_test.shape: torch.Size([26032, 3, 32, 32])
cle_y_test.shape: torch.Size([26032])
adv_x_test.shape: torch.Size([10000, 3, 32, 32])
adv_y_test.shape: torch.Size([10000])
cle_train_dataloader.len: 287
Accuary of before rmt trained classifier on adversarial testset:0.6200%
Loss of before mmat trained classifier on adversarial testset:13.3988618850708
w_trainset_len: 24004
batch_size: 256
w_batch_num: 94


0epoch learning rate:0.001
[Epoch 1/40] [Batch 66/287] [Batch classify loss: 1.140843]
Setting up PyTorch plugin "bias_act_plugin"... /home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/utils/cpp_extension.py:286: UserWarning: 

                               !! WARNING !!

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Your compiler (c++) is not compatible with the compiler Pytorch was
built with for this platform, which is g++ on linux. Please
use g++ to to compile your extension. Alternatively, you may
compile PyTorch from source using c++, and then you can also use
c++ to compile your extension.

See https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md for help
with compiling PyTorch from source.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

                              !! WARNING !!

  platform=sys.platform))
Done.
Setting up PyTorch plugin "upfirdn2d_plugin"... /home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/utils/cpp_extension.py:286: UserWarning: 

                               !! WARNING !!

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Your compiler (c++) is not compatible with the compiler Pytorch was
built with for this platform, which is g++ on linux. Please
use g++ to to compile your extension. Alternatively, you may
compile PyTorch from source using c++, and then you can also use
c++ to compile your extension.

See https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md for help
with compiling PyTorch from source.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

                              !! WARNING !!

  platform=sys.platform))
Done.
[Epoch 1/40] [Batch 67/287] [Batch classify loss: 1.102169]
[Epoch 1/40] [Batch 1/287] [Batch classify loss: 2.353411]
[Epoch 1/40] [Batch 68/287] [Batch classify loss: 1.249368]
[Epoch 1/40] [Batch 69/287] [Batch classify loss: 1.169989]
[Epoch 1/40] [Batch 2/287] [Batch classify loss: 2.147465]
[Epoch 1/40] [Batch 70/287] [Batch classify loss: 1.076899]
[Epoch 1/40] [Batch 3/287] [Batch classify loss: 1.901062]
[Epoch 1/40] [Batch 71/287] [Batch classify loss: 1.164425]
[Epoch 1/40] [Batch 4/287] [Batch classify loss: 1.689772]
[Epoch 1/40] [Batch 72/287] [Batch classify loss: 1.107066]
[Epoch 1/40] [Batch 73/287] [Batch classify loss: 1.170979]
[Epoch 1/40] [Batch 5/287] [Batch classify loss: 1.561986]
[Epoch 1/40] [Batch 74/287] [Batch classify loss: 1.117042]
[Epoch 1/40] [Batch 6/287] [Batch classify loss: 1.331002]
[Epoch 1/40] [Batch 75/287] [Batch classify loss: 1.128486]
[Epoch 1/40] [Batch 7/287] [Batch classify loss: 1.343450]
[Epoch 1/40] [Batch 76/287] [Batch classify loss: 1.222632]
[Epoch 1/40] [Batch 77/287] [Batch classify loss: 1.203992]
[Epoch 1/40] [Batch 8/287] [Batch classify loss: 1.257738]
[Epoch 1/40] [Batch 78/287] [Batch classify loss: 1.129934]
[Epoch 1/40] [Batch 9/287] [Batch classify loss: 1.412434]
[Epoch 1/40] [Batch 79/287] [Batch classify loss: 1.213044]
[Epoch 1/40] [Batch 10/287] [Batch classify loss: 1.260774]
[Epoch 1/40] [Batch 80/287] [Batch classify loss: 1.174914]
[Epoch 1/40] [Batch 81/287] [Batch classify loss: 1.153610]
[Epoch 1/40] [Batch 11/287] [Batch classify loss: 1.340543]
[Epoch 1/40] [Batch 82/287] [Batch classify loss: 1.119689]
[Epoch 1/40] [Batch 12/287] [Batch classify loss: 1.237404]
[Epoch 1/40] [Batch 83/287] [Batch classify loss: 1.113853]
[Epoch 1/40] [Batch 13/287] [Batch classify loss: 1.294030]
[Epoch 1/40] [Batch 84/287] [Batch classify loss: 1.116249]
[Epoch 1/40] [Batch 85/287] [Batch classify loss: 1.156108]
[Epoch 1/40] [Batch 14/287] [Batch classify loss: 1.272515]
[Epoch 1/40] [Batch 86/287] [Batch classify loss: 1.184141]
[Epoch 1/40] [Batch 15/287] [Batch classify loss: 1.205030]
[Epoch 1/40] [Batch 87/287] [Batch classify loss: 1.121292]
[Epoch 1/40] [Batch 16/287] [Batch classify loss: 1.208094]
[Epoch 1/40] [Batch 88/287] [Batch classify loss: 1.201504]
[Epoch 1/40] [Batch 17/287] [Batch classify loss: 1.245754]
[Epoch 1/40] [Batch 89/287] [Batch classify loss: 1.178424]
[Epoch 1/40] [Batch 90/287] [Batch classify loss: 1.109050]
[Epoch 1/40] [Batch 18/287] [Batch classify loss: 1.216055]
[Epoch 1/40] [Batch 91/287] [Batch classify loss: 1.192044]
[Epoch 1/40] [Batch 19/287] [Batch classify loss: 1.189390]
[Epoch 1/40] [Batch 92/287] [Batch classify loss: 1.120493]
[Epoch 1/40] [Batch 20/287] [Batch classify loss: 1.232705]
[Epoch 1/40] [Batch 93/287] [Batch classify loss: 1.152622]
[Epoch 1/40] [Batch 21/287] [Batch classify loss: 1.198249]
[Epoch 1/40] [Batch 94/287] [Batch classify loss: 1.079779]
[Epoch 1/40] [Batch 95/287] [Batch classify loss: 1.196644]
[Epoch 1/40] [Batch 22/287] [Batch classify loss: 1.193116]
[Epoch 1/40] [Batch 96/287] [Batch classify loss: 1.078697]
[Epoch 1/40] [Batch 23/287] [Batch classify loss: 1.272799]
[Epoch 1/40] [Batch 97/287] [Batch classify loss: 1.161312]
[Epoch 1/40] [Batch 24/287] [Batch classify loss: 1.219143]
[Epoch 1/40] [Batch 98/287] [Batch classify loss: 1.125883]
[Epoch 1/40] [Batch 25/287] [Batch classify loss: 1.199841]
[Epoch 1/40] [Batch 99/287] [Batch classify loss: 1.100924]
[Epoch 1/40] [Batch 100/287] [Batch classify loss: 1.204410]
[Epoch 1/40] [Batch 26/287] [Batch classify loss: 1.170954]
[Epoch 1/40] [Batch 101/287] [Batch classify loss: 1.204490]
[Epoch 1/40] [Batch 27/287] [Batch classify loss: 1.139778]
[Epoch 1/40] [Batch 102/287] [Batch classify loss: 1.203249]
[Epoch 1/40] [Batch 28/287] [Batch classify loss: 1.173336]
[Epoch 1/40] [Batch 103/287] [Batch classify loss: 1.148009]
[Epoch 1/40] [Batch 29/287] [Batch classify loss: 1.236343]
[Epoch 1/40] [Batch 104/287] [Batch classify loss: 1.132639]
[Epoch 1/40] [Batch 105/287] [Batch classify loss: 1.129073]
[Epoch 1/40] [Batch 30/287] [Batch classify loss: 1.205903]
[Epoch 1/40] [Batch 106/287] [Batch classify loss: 1.176472]
[Epoch 1/40] [Batch 31/287] [Batch classify loss: 1.245717]
[Epoch 1/40] [Batch 107/287] [Batch classify loss: 1.051491]
[Epoch 1/40] [Batch 32/287] [Batch classify loss: 1.151591]
[Epoch 1/40] [Batch 108/287] [Batch classify loss: 1.063394]
[Epoch 1/40] [Batch 109/287] [Batch classify loss: 1.157682]
[Epoch 1/40] [Batch 33/287] [Batch classify loss: 1.097932]
[Epoch 1/40] [Batch 110/287] [Batch classify loss: 1.128315]
[Epoch 1/40] [Batch 34/287] [Batch classify loss: 1.165027]
[Epoch 1/40] [Batch 111/287] [Batch classify loss: 1.136859]
[Epoch 1/40] [Batch 35/287] [Batch classify loss: 1.217278]
[Epoch 1/40] [Batch 112/287] [Batch classify loss: 1.128752]
[Epoch 1/40] [Batch 36/287] [Batch classify loss: 1.221220]
[Epoch 1/40] [Batch 113/287] [Batch classify loss: 1.131383]
[Epoch 1/40] [Batch 37/287] [Batch classify loss: 1.186568]
[Epoch 1/40] [Batch 114/287] [Batch classify loss: 1.138939]
[Epoch 1/40] [Batch 115/287] [Batch classify loss: 1.124483]
[Epoch 1/40] [Batch 38/287] [Batch classify loss: 1.190508]
[Epoch 1/40] [Batch 116/287] [Batch classify loss: 1.152643]
[Epoch 1/40] [Batch 39/287] [Batch classify loss: 1.138266]
[Epoch 1/40] [Batch 117/287] [Batch classify loss: 1.149873]
[Epoch 1/40] [Batch 40/287] [Batch classify loss: 1.156926]
[Epoch 1/40] [Batch 118/287] [Batch classify loss: 1.132514]
[Epoch 1/40] [Batch 119/287] [Batch classify loss: 1.101536]
[Epoch 1/40] [Batch 41/287] [Batch classify loss: 1.190768]
[Epoch 1/40] [Batch 120/287] [Batch classify loss: 1.134731]
[Epoch 1/40] [Batch 42/287] [Batch classify loss: 1.146871]
[Epoch 1/40] [Batch 121/287] [Batch classify loss: 1.046974]
[Epoch 1/40] [Batch 43/287] [Batch classify loss: 1.209317]
[Epoch 1/40] [Batch 122/287] [Batch classify loss: 1.157655]
[Epoch 1/40] [Batch 44/287] [Batch classify loss: 1.180048]
[Epoch 1/40] [Batch 123/287] [Batch classify loss: 1.196962]
[Epoch 1/40] [Batch 45/287] [Batch classify loss: 1.177528]
[Epoch 1/40] [Batch 124/287] [Batch classify loss: 1.171672]
[Epoch 1/40] [Batch 125/287] [Batch classify loss: 1.167737]
[Epoch 1/40] [Batch 46/287] [Batch classify loss: 1.147525]
[Epoch 1/40] [Batch 126/287] [Batch classify loss: 1.150786]
[Epoch 1/40] [Batch 47/287] [Batch classify loss: 1.141121]
[Epoch 1/40] [Batch 127/287] [Batch classify loss: 1.126997]
[Epoch 1/40] [Batch 48/287] [Batch classify loss: 1.160293]
[Epoch 1/40] [Batch 128/287] [Batch classify loss: 1.204509]
[Epoch 1/40] [Batch 129/287] [Batch classify loss: 1.162979]
[Epoch 1/40] [Batch 49/287] [Batch classify loss: 1.160748]
[Epoch 1/40] [Batch 130/287] [Batch classify loss: 1.143752]
[Epoch 1/40] [Batch 50/287] [Batch classify loss: 1.159526]
[Epoch 1/40] [Batch 131/287] [Batch classify loss: 1.089450]
[Epoch 1/40] [Batch 51/287] [Batch classify loss: 1.145724]
[Epoch 1/40] [Batch 132/287] [Batch classify loss: 1.152012]
[Epoch 1/40] [Batch 133/287] [Batch classify loss: 1.160511]
[Epoch 1/40] [Batch 52/287] [Batch classify loss: 1.182981]
[Epoch 1/40] [Batch 134/287] [Batch classify loss: 1.112637]
[Epoch 1/40] [Batch 53/287] [Batch classify loss: 1.271764]
[Epoch 1/40] [Batch 135/287] [Batch classify loss: 1.109322]
[Epoch 1/40] [Batch 54/287] [Batch classify loss: 1.226727]
[Epoch 1/40] [Batch 136/287] [Batch classify loss: 1.177824]
[Epoch 1/40] [Batch 137/287] [Batch classify loss: 1.040300]
[Epoch 1/40] [Batch 55/287] [Batch classify loss: 1.102730]
[Epoch 1/40] [Batch 138/287] [Batch classify loss: 1.135952]
[Epoch 1/40] [Batch 56/287] [Batch classify loss: 1.191660]
[Epoch 1/40] [Batch 139/287] [Batch classify loss: 1.142867]
[Epoch 1/40] [Batch 57/287] [Batch classify loss: 1.166215]
[Epoch 1/40] [Batch 140/287] [Batch classify loss: 1.133398]
[Epoch 1/40] [Batch 141/287] [Batch classify loss: 1.100933]
[Epoch 1/40] [Batch 58/287] [Batch classify loss: 1.161801]
[Epoch 1/40] [Batch 142/287] [Batch classify loss: 1.194879]
[Epoch 1/40] [Batch 59/287] [Batch classify loss: 1.123193]
[Epoch 1/40] [Batch 143/287] [Batch classify loss: 1.108307]
[Epoch 1/40] [Batch 60/287] [Batch classify loss: 1.156741]
[Epoch 1/40] [Batch 144/287] [Batch classify loss: 1.197216]
[Epoch 1/40] [Batch 61/287] [Batch classify loss: 1.130924]
[Epoch 1/40] [Batch 145/287] [Batch classify loss: 1.196614]
[Epoch 1/40] [Batch 146/287] [Batch classify loss: 1.132944]
[Epoch 1/40] [Batch 62/287] [Batch classify loss: 1.214950]
[Epoch 1/40] [Batch 147/287] [Batch classify loss: 1.130745]
[Epoch 1/40] [Batch 63/287] [Batch classify loss: 1.139267]
[Epoch 1/40] [Batch 148/287] [Batch classify loss: 1.088826]
[Epoch 1/40] [Batch 64/287] [Batch classify loss: 1.193909]
[Epoch 1/40] [Batch 149/287] [Batch classify loss: 1.092722]
[Epoch 1/40] [Batch 65/287] [Batch classify loss: 1.131231]
[Epoch 1/40] [Batch 150/287] [Batch classify loss: 1.155977]
[Epoch 1/40] [Batch 151/287] [Batch classify loss: 1.132684]
[Epoch 1/40] [Batch 66/287] [Batch classify loss: 1.206318]
[Epoch 1/40] [Batch 152/287] [Batch classify loss: 1.153234]
[Epoch 1/40] [Batch 67/287] [Batch classify loss: 1.189986]
[Epoch 1/40] [Batch 153/287] [Batch classify loss: 1.145885]
[Epoch 1/40] [Batch 68/287] [Batch classify loss: 1.140296]
[Epoch 1/40] [Batch 154/287] [Batch classify loss: 1.180144]
[Epoch 1/40] [Batch 69/287] [Batch classify loss: 1.085989]
[Epoch 1/40] [Batch 155/287] [Batch classify loss: 1.111566]
[Epoch 1/40] [Batch 156/287] [Batch classify loss: 1.163794]
[Epoch 1/40] [Batch 70/287] [Batch classify loss: 1.132215]
[Epoch 1/40] [Batch 157/287] [Batch classify loss: 1.135545]
[Epoch 1/40] [Batch 71/287] [Batch classify loss: 1.171698]
[Epoch 1/40] [Batch 158/287] [Batch classify loss: 1.152802]
[Epoch 1/40] [Batch 72/287] [Batch classify loss: 1.149217]
[Epoch 1/40] [Batch 159/287] [Batch classify loss: 1.165223]
[Epoch 1/40] [Batch 73/287] [Batch classify loss: 1.199722]
[Epoch 1/40] [Batch 160/287] [Batch classify loss: 1.173340]
[Epoch 1/40] [Batch 74/287] [Batch classify loss: 1.081404]
[Epoch 1/40] [Batch 161/287] [Batch classify loss: 1.151407]
[Epoch 1/40] [Batch 162/287] [Batch classify loss: 1.146296]
[Epoch 1/40] [Batch 75/287] [Batch classify loss: 1.171375]
[Epoch 1/40] [Batch 163/287] [Batch classify loss: 1.134506]
[Epoch 1/40] [Batch 76/287] [Batch classify loss: 1.141119]
[Epoch 1/40] [Batch 164/287] [Batch classify loss: 1.104505]
[Epoch 1/40] [Batch 165/287] [Batch classify loss: 1.175400]
[Epoch 1/40] [Batch 77/287] [Batch classify loss: 1.143351]
[Epoch 1/40] [Batch 166/287] [Batch classify loss: 1.122539]
[Epoch 1/40] [Batch 78/287] [Batch classify loss: 1.176175]
[Epoch 1/40] [Batch 167/287] [Batch classify loss: 1.129715]
[Epoch 1/40] [Batch 79/287] [Batch classify loss: 1.176412]
[Epoch 1/40] [Batch 168/287] [Batch classify loss: 1.206418]
[Epoch 1/40] [Batch 80/287] [Batch classify loss: 1.098550]
[Epoch 1/40] [Batch 169/287] [Batch classify loss: 1.126441]
[Epoch 1/40] [Batch 81/287] [Batch classify loss: 1.152173]
[Epoch 1/40] [Batch 170/287] [Batch classify loss: 1.101814]
[Epoch 1/40] [Batch 171/287] [Batch classify loss: 1.176653]
[Epoch 1/40] [Batch 82/287] [Batch classify loss: 1.147052]
[Epoch 1/40] [Batch 172/287] [Batch classify loss: 1.219307]
[Epoch 1/40] [Batch 83/287] [Batch classify loss: 1.100788]
[Epoch 1/40] [Batch 173/287] [Batch classify loss: 1.131231]
[Epoch 1/40] [Batch 84/287] [Batch classify loss: 1.129917]
[Epoch 1/40] [Batch 174/287] [Batch classify loss: 1.149601]
[Epoch 1/40] [Batch 85/287] [Batch classify loss: 1.132966]
[Epoch 1/40] [Batch 175/287] [Batch classify loss: 1.082166]
[Epoch 1/40] [Batch 86/287] [Batch classify loss: 1.169273]
[Epoch 1/40] [Batch 176/287] [Batch classify loss: 1.104025]
[Epoch 1/40] [Batch 177/287] [Batch classify loss: 1.118589]
[Epoch 1/40] [Batch 87/287] [Batch classify loss: 1.124848]
[Epoch 1/40] [Batch 178/287] [Batch classify loss: 1.157085]
[Epoch 1/40] [Batch 88/287] [Batch classify loss: 1.086079]
