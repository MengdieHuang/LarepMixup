

---------------------------------------
Torch cuda is available
args.subcommand=run, run the command line
date: 20220114
blackbox attack
Experiment result save dir: /home/maggie/mmat/result/defense/rmt/om-pgd/basemixup-betasampler/resnet34-svhn/blackbox/20220114/00000
initilize the dataset loading parameters
Using downloaded and verified file: /home/data/maggie/svhn/train_32x32.mat
Loading *svhn* train dataloader finished !
Loading *svhn* test dataloader finished !
initlize classifier
learned calssify model != None
args.adv_datasetï¼š /home/maggie/mmat/result/attack/pgd/resnet34-svhn/20220113/00002-ompgd-eps-0.02-acc-64.7300/attack-svhn-dataset/latent-attack-samples
Accuary of before rmt trained classifier on clean testset:93.4312%
Loss of before mmat trained classifier clean testset:0.25741657614707947
args.mix_mode: basemixup
args.mix_w_num: 2
args.beta_alpha: 2.0
args.dirichlet_gama: 1
cle_w_train.shape: torch.Size([24004, 8, 512])
cle_y_train.shape: torch.Size([24004, 8])
cle_x_test.shape: torch.Size([26032, 3, 32, 32])
cle_y_test.shape: torch.Size([26032])
adv_x_test.shape: torch.Size([10000, 3, 32, 32])
adv_y_test.shape: torch.Size([10000])
cle_train_dataloader.len: 287
Accuary of before rmt trained classifier on adversarial testset:64.7300%
Loss of before mmat trained classifier on adversarial testset:2.06988787651062
w_trainset_len: 24004
batch_size: 256
w_batch_num: 94


0epoch learning rate:0.001
Setting up PyTorch plugin "bias_act_plugin"... /home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/utils/cpp_extension.py:286: UserWarning: 

                               !! WARNING !!

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Your compiler (c++) is not compatible with the compiler Pytorch was
built with for this platform, which is g++ on linux. Please
use g++ to to compile your extension. Alternatively, you may
compile PyTorch from source using c++, and then you can also use
c++ to compile your extension.

See https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md for help
with compiling PyTorch from source.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

                              !! WARNING !!

  platform=sys.platform))
Done.
Setting up PyTorch plugin "upfirdn2d_plugin"... /home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/utils/cpp_extension.py:286: UserWarning: 

                               !! WARNING !!

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Your compiler (c++) is not compatible with the compiler Pytorch was
built with for this platform, which is g++ on linux. Please
use g++ to to compile your extension. Alternatively, you may
compile PyTorch from source using c++, and then you can also use
c++ to compile your extension.

See https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md for help
with compiling PyTorch from source.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

                              !! WARNING !!

  platform=sys.platform))
Done.
[Epoch 1/40] [Batch 1/287] [Batch classify loss: 2.615575]
[Epoch 1/40] [Batch 2/287] [Batch classify loss: 1.842868]
[Epoch 1/40] [Batch 3/287] [Batch classify loss: 1.696583]
[Epoch 1/40] [Batch 4/287] [Batch classify loss: 1.439428]
[Epoch 1/40] [Batch 5/287] [Batch classify loss: 1.377512]
[Epoch 1/40] [Batch 6/287] [Batch classify loss: 1.418684]
[Epoch 1/40] [Batch 7/287] [Batch classify loss: 1.392269]
[Epoch 1/40] [Batch 8/287] [Batch classify loss: 1.391797]
[Epoch 1/40] [Batch 9/287] [Batch classify loss: 1.335289]
[Epoch 1/40] [Batch 10/287] [Batch classify loss: 1.231192]
[Epoch 1/40] [Batch 11/287] [Batch classify loss: 1.239655]
[Epoch 1/40] [Batch 12/287] [Batch classify loss: 1.256079]
[Epoch 1/40] [Batch 13/287] [Batch classify loss: 1.174734]
[Epoch 1/40] [Batch 14/287] [Batch classify loss: 1.236364]
[Epoch 1/40] [Batch 15/287] [Batch classify loss: 1.233356]
[Epoch 1/40] [Batch 16/287] [Batch classify loss: 1.158812]
[Epoch 1/40] [Batch 17/287] [Batch classify loss: 1.229574]
[Epoch 1/40] [Batch 18/287] [Batch classify loss: 1.184792]
[Epoch 1/40] [Batch 19/287] [Batch classify loss: 1.193282]
[Epoch 1/40] [Batch 20/287] [Batch classify loss: 1.127103]
[Epoch 1/40] [Batch 21/287] [Batch classify loss: 1.145349]
[Epoch 1/40] [Batch 22/287] [Batch classify loss: 1.188214]
[Epoch 1/40] [Batch 23/287] [Batch classify loss: 1.167200]
[Epoch 1/40] [Batch 24/287] [Batch classify loss: 1.160314]
[Epoch 1/40] [Batch 25/287] [Batch classify loss: 1.140631]
[Epoch 1/40] [Batch 26/287] [Batch classify loss: 1.203045]
[Epoch 1/40] [Batch 27/287] [Batch classify loss: 1.143763]
[Epoch 1/40] [Batch 28/287] [Batch classify loss: 1.083385]
[Epoch 1/40] [Batch 29/287] [Batch classify loss: 1.174789]
[Epoch 1/40] [Batch 30/287] [Batch classify loss: 1.175883]
[Epoch 1/40] [Batch 31/287] [Batch classify loss: 1.190752]
[Epoch 1/40] [Batch 32/287] [Batch classify loss: 1.176594]
[Epoch 1/40] [Batch 33/287] [Batch classify loss: 1.191071]
[Epoch 1/40] [Batch 34/287] [Batch classify loss: 1.161652]
[Epoch 1/40] [Batch 35/287] [Batch classify loss: 1.179712]
[Epoch 1/40] [Batch 36/287] [Batch classify loss: 1.179014]
[Epoch 1/40] [Batch 37/287] [Batch classify loss: 1.124430]
[Epoch 1/40] [Batch 38/287] [Batch classify loss: 1.181743]
[Epoch 1/40] [Batch 39/287] [Batch classify loss: 1.164698]
[Epoch 1/40] [Batch 40/287] [Batch classify loss: 1.156614]
[Epoch 1/40] [Batch 41/287] [Batch classify loss: 1.162459]
[Epoch 1/40] [Batch 42/287] [Batch classify loss: 1.155151]
[Epoch 1/40] [Batch 43/287] [Batch classify loss: 1.174498]
[Epoch 1/40] [Batch 44/287] [Batch classify loss: 1.159483]
[Epoch 1/40] [Batch 45/287] [Batch classify loss: 1.090513]
[Epoch 1/40] [Batch 46/287] [Batch classify loss: 1.131053]
[Epoch 1/40] [Batch 47/287] [Batch classify loss: 1.128471]
[Epoch 1/40] [Batch 48/287] [Batch classify loss: 1.087680]
[Epoch 1/40] [Batch 49/287] [Batch classify loss: 1.087629]
[Epoch 1/40] [Batch 50/287] [Batch classify loss: 1.182650]
[Epoch 1/40] [Batch 51/287] [Batch classify loss: 1.125000]
[Epoch 1/40] [Batch 52/287] [Batch classify loss: 1.168216]
[Epoch 1/40] [Batch 53/287] [Batch classify loss: 1.213238]
[Epoch 1/40] [Batch 54/287] [Batch classify loss: 1.133228]
[Epoch 1/40] [Batch 55/287] [Batch classify loss: 1.125271]
[Epoch 1/40] [Batch 56/287] [Batch classify loss: 1.156629]
[Epoch 1/40] [Batch 57/287] [Batch classify loss: 1.159925]
[Epoch 1/40] [Batch 58/287] [Batch classify loss: 1.122070]
[Epoch 1/40] [Batch 59/287] [Batch classify loss: 1.088839]
[Epoch 1/40] [Batch 60/287] [Batch classify loss: 1.135682]
[Epoch 1/40] [Batch 61/287] [Batch classify loss: 1.176248]
[Epoch 1/40] [Batch 62/287] [Batch classify loss: 1.135124]
[Epoch 1/40] [Batch 63/287] [Batch classify loss: 1.153058]
[Epoch 1/40] [Batch 64/287] [Batch classify loss: 1.140841]
[Epoch 1/40] [Batch 65/287] [Batch classify loss: 1.166229]
[Epoch 1/40] [Batch 66/287] [Batch classify loss: 1.166360]
[Epoch 1/40] [Batch 67/287] [Batch classify loss: 1.144349]
[Epoch 1/40] [Batch 68/287] [Batch classify loss: 1.111634]
[Epoch 1/40] [Batch 69/287] [Batch classify loss: 1.148638]
[Epoch 1/40] [Batch 70/287] [Batch classify loss: 1.115843]
[Epoch 1/40] [Batch 71/287] [Batch classify loss: 1.130031]
[Epoch 1/40] [Batch 72/287] [Batch classify loss: 1.113463]
[Epoch 1/40] [Batch 73/287] [Batch classify loss: 1.102723]
[Epoch 1/40] [Batch 74/287] [Batch classify loss: 1.152095]
[Epoch 1/40] [Batch 75/287] [Batch classify loss: 1.085052]
[Epoch 1/40] [Batch 76/287] [Batch classify loss: 1.103610]
[Epoch 1/40] [Batch 77/287] [Batch classify loss: 1.125106]
[Epoch 1/40] [Batch 78/287] [Batch classify loss: 1.097701]
[Epoch 1/40] [Batch 79/287] [Batch classify loss: 1.169584]
[Epoch 1/40] [Batch 80/287] [Batch classify loss: 1.100371]
[Epoch 1/40] [Batch 81/287] [Batch classify loss: 1.073454]
[Epoch 1/40] [Batch 82/287] [Batch classify loss: 1.168250]
[Epoch 1/40] [Batch 83/287] [Batch classify loss: 1.059236]
[Epoch 1/40] [Batch 84/287] [Batch classify loss: 1.082325]
[Epoch 1/40] [Batch 85/287] [Batch classify loss: 1.127914]
[Epoch 1/40] [Batch 86/287] [Batch classify loss: 1.137417]
[Epoch 1/40] [Batch 87/287] [Batch classify loss: 1.152010]
[Epoch 1/40] [Batch 88/287] [Batch classify loss: 1.115683]
[Epoch 1/40] [Batch 89/287] [Batch classify loss: 1.111533]
[Epoch 1/40] [Batch 90/287] [Batch classify loss: 1.103290]
[Epoch 1/40] [Batch 91/287] [Batch classify loss: 1.108039]
[Epoch 1/40] [Batch 92/287] [Batch classify loss: 1.133808]
[Epoch 1/40] [Batch 93/287] [Batch classify loss: 1.119338]
[Epoch 1/40] [Batch 94/287] [Batch classify loss: 1.018227]
[Epoch 1/40] [Batch 95/287] [Batch classify loss: 1.129115]
[Epoch 1/40] [Batch 96/287] [Batch classify loss: 1.048966]
[Epoch 1/40] [Batch 97/287] [Batch classify loss: 1.185034]
[Epoch 1/40] [Batch 98/287] [Batch classify loss: 1.086688]
[Epoch 1/40] [Batch 99/287] [Batch classify loss: 1.170801]
[Epoch 1/40] [Batch 100/287] [Batch classify loss: 1.149084]
[Epoch 1/40] [Batch 101/287] [Batch classify loss: 1.090172]
[Epoch 1/40] [Batch 102/287] [Batch classify loss: 1.157780]
[Epoch 1/40] [Batch 103/287] [Batch classify loss: 1.123806]
[Epoch 1/40] [Batch 104/287] [Batch classify loss: 1.076253]
[Epoch 1/40] [Batch 105/287] [Batch classify loss: 1.027933]
[Epoch 1/40] [Batch 106/287] [Batch classify loss: 1.126831]
[Epoch 1/40] [Batch 107/287] [Batch classify loss: 1.058701]
[Epoch 1/40] [Batch 108/287] [Batch classify loss: 1.144156]
[Epoch 1/40] [Batch 109/287] [Batch classify loss: 1.136534]
[Epoch 1/40] [Batch 110/287] [Batch classify loss: 1.188610]
[Epoch 1/40] [Batch 111/287] [Batch classify loss: 1.127894]
[Epoch 1/40] [Batch 112/287] [Batch classify loss: 1.068092]
[Epoch 1/40] [Batch 113/287] [Batch classify loss: 1.055440]
[Epoch 1/40] [Batch 114/287] [Batch classify loss: 1.094951]
[Epoch 1/40] [Batch 115/287] [Batch classify loss: 1.098434]
[Epoch 1/40] [Batch 116/287] [Batch classify loss: 1.081494]
[Epoch 1/40] [Batch 117/287] [Batch classify loss: 1.123520]
[Epoch 1/40] [Batch 118/287] [Batch classify loss: 1.065588]
[Epoch 1/40] [Batch 119/287] [Batch classify loss: 1.132198]
[Epoch 1/40] [Batch 120/287] [Batch classify loss: 1.113760]
[Epoch 1/40] [Batch 121/287] [Batch classify loss: 1.116189]
[Epoch 1/40] [Batch 122/287] [Batch classify loss: 1.120713]
[Epoch 1/40] [Batch 123/287] [Batch classify loss: 1.076364]
[Epoch 1/40] [Batch 124/287] [Batch classify loss: 1.098871]
[Epoch 1/40] [Batch 125/287] [Batch classify loss: 1.093528]
[Epoch 1/40] [Batch 126/287] [Batch classify loss: 1.115555]
[Epoch 1/40] [Batch 127/287] [Batch classify loss: 1.091378]
[Epoch 1/40] [Batch 128/287] [Batch classify loss: 1.062698]
[Epoch 1/40] [Batch 129/287] [Batch classify loss: 1.030134]
[Epoch 1/40] [Batch 130/287] [Batch classify loss: 1.126273]
[Epoch 1/40] [Batch 131/287] [Batch classify loss: 1.140959]
[Epoch 1/40] [Batch 132/287] [Batch classify loss: 1.157617]
[Epoch 1/40] [Batch 133/287] [Batch classify loss: 1.136645]
[Epoch 1/40] [Batch 134/287] [Batch classify loss: 1.100060]
[Epoch 1/40] [Batch 135/287] [Batch classify loss: 1.130499]
[Epoch 1/40] [Batch 136/287] [Batch classify loss: 1.041917]
[Epoch 1/40] [Batch 137/287] [Batch classify loss: 1.145524]
[Epoch 1/40] [Batch 138/287] [Batch classify loss: 1.113004]
[Epoch 1/40] [Batch 139/287] [Batch classify loss: 1.110741]
[Epoch 1/40] [Batch 140/287] [Batch classify loss: 1.150635]
[Epoch 1/40] [Batch 141/287] [Batch classify loss: 1.085725]
[Epoch 1/40] [Batch 142/287] [Batch classify loss: 1.129872]
[Epoch 1/40] [Batch 143/287] [Batch classify loss: 1.102822]
[Epoch 1/40] [Batch 144/287] [Batch classify loss: 1.156221]
[Epoch 1/40] [Batch 145/287] [Batch classify loss: 1.080178]
[Epoch 1/40] [Batch 146/287] [Batch classify loss: 1.068571]
[Epoch 1/40] [Batch 147/287] [Batch classify loss: 1.097628]
[Epoch 1/40] [Batch 148/287] [Batch classify loss: 1.113509]
[Epoch 1/40] [Batch 149/287] [Batch classify loss: 1.061780]
[Epoch 1/40] [Batch 150/287] [Batch classify loss: 1.085018]
[Epoch 1/40] [Batch 151/287] [Batch classify loss: 1.128220]
[Epoch 1/40] [Batch 152/287] [Batch classify loss: 1.137004]
[Epoch 1/40] [Batch 153/287] [Batch classify loss: 1.097777]
[Epoch 1/40] [Batch 154/287] [Batch classify loss: 1.116117]
[Epoch 1/40] [Batch 155/287] [Batch classify loss: 1.185060]
[Epoch 1/40] [Batch 156/287] [Batch classify loss: 1.124565]
[Epoch 1/40] [Batch 157/287] [Batch classify loss: 1.072213]
[Epoch 1/40] [Batch 158/287] [Batch classify loss: 1.106798]
[Epoch 1/40] [Batch 159/287] [Batch classify loss: 1.145978]
[Epoch 1/40] [Batch 160/287] [Batch classify loss: 1.118921]
[Epoch 1/40] [Batch 161/287] [Batch classify loss: 1.070803]
[Epoch 1/40] [Batch 162/287] [Batch classify loss: 1.099941]
[Epoch 1/40] [Batch 163/287] [Batch classify loss: 1.125756]
[Epoch 1/40] [Batch 164/287] [Batch classify loss: 1.099338]
[Epoch 1/40] [Batch 165/287] [Batch classify loss: 1.129035]
[Epoch 1/40] [Batch 166/287] [Batch classify loss: 1.159037]
[Epoch 1/40] [Batch 167/287] [Batch classify loss: 1.104894]
[Epoch 1/40] [Batch 168/287] [Batch classify loss: 1.076780]
[Epoch 1/40] [Batch 169/287] [Batch classify loss: 1.085994]
[Epoch 1/40] [Batch 170/287] [Batch classify loss: 1.123248]
[Epoch 1/40] [Batch 171/287] [Batch classify loss: 1.086112]
[Epoch 1/40] [Batch 172/287] [Batch classify loss: 1.117345]
[Epoch 1/40] [Batch 173/287] [Batch classify loss: 1.028016]
[Epoch 1/40] [Batch 174/287] [Batch classify loss: 1.138771]
[Epoch 1/40] [Batch 175/287] [Batch classify loss: 1.120749]
[Epoch 1/40] [Batch 176/287] [Batch classify loss: 1.110334]
[Epoch 1/40] [Batch 177/287] [Batch classify loss: 1.034794]
[Epoch 1/40] [Batch 178/287] [Batch classify loss: 1.105403]
