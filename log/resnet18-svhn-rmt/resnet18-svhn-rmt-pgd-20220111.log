

---------------------------------------
Torch cuda is available
args.subcommand=run, run the command line
date: 20220111
whitebox attack
Experiment result save dir: /home/maggie/mmat/result/defense/rmt/pgd/basemixup-betasampler/resnet18-svhn/whitebox/20220111/00000
initilize the dataset loading parameters
Using downloaded and verified file: /home/data/maggie/svhn/train_32x32.mat
Loading *svhn* train dataloader finished !
Loading *svhn* test dataloader finished !
initlize classifier
learned calssify model != None
args.adv_dataset： /home/maggie/mmat/result/attack/pgd/resnet18-svhn/20210914/00000-attackacc-0.166756/attack-svhn-dataset/samples
Accuary of before rmt trained classifier on clean testset:93.3774%
Loss of before mmat trained classifier clean testset:0.25881198048591614
args.mix_mode: basemixup
args.mix_w_num: 2
args.beta_alpha: 0.5
args.dirichlet_gama: 1
cle_w_train.shape: torch.Size([24004, 8, 512])
cle_y_train.shape: torch.Size([24004, 8])
cle_x_test.shape: torch.Size([26032, 3, 32, 32])
cle_y_test.shape: torch.Size([26032])
adv_x_test.shape: torch.Size([26032, 3, 32, 32])
adv_y_test.shape: torch.Size([26032])
cle_train_dataloader.len: 287
initlize attack classifier
generate pixel adversarial exampels
generating testset adversarial examples...
PGD - Batches:   0%|          | 0/814 [00:00<?, ?it/s]/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/art/estimators/classification/pytorch.py:1143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  x_grad = torch.tensor(x).to(self._device)
/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/art/estimators/classification/pytorch.py:1144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  y_grad = torch.tensor(y).to(self._device)
PGD - Batches:   0%|          | 1/814 [00:01<22:40,  1.67s/it]PGD - Batches:   0%|          | 2/814 [00:03<22:40,  1.67s/it]PGD - Batches:   0%|          | 3/814 [00:05<22:50,  1.69s/it]PGD - Batches:   0%|          | 4/814 [00:06<22:42,  1.68s/it]PGD - Batches:   1%|          | 5/814 [00:08<22:32,  1.67s/it]PGD - Batches:   1%|          | 6/814 [00:10<22:43,  1.69s/it]PGD - Batches:   1%|          | 7/814 [00:11<22:18,  1.66s/it]PGD - Batches:   1%|          | 8/814 [00:13<22:11,  1.65s/it]PGD - Batches:   1%|          | 9/814 [00:14<21:56,  1.64s/it]PGD - Batches:   1%|          | 10/814 [00:16<22:10,  1.65s/it]PGD - Batches:   1%|▏         | 11/814 [00:18<21:59,  1.64s/it]PGD - Batches:   1%|▏         | 12/814 [00:19<22:03,  1.65s/it]PGD - Batches:   2%|▏         | 13/814 [00:21<21:49,  1.63s/it]PGD - Batches:   2%|▏         | 14/814 [00:23<21:42,  1.63s/it]PGD - Batches:   2%|▏         | 15/814 [00:24<21:29,  1.61s/it]PGD - Batches:   2%|▏         | 16/814 [00:26<21:32,  1.62s/it]PGD - Batches:   2%|▏         | 17/814 [00:27<21:29,  1.62s/it]PGD - Batches:   2%|▏         | 18/814 [00:29<21:37,  1.63s/it]PGD - Batches:   2%|▏         | 19/814 [00:31<21:40,  1.64s/it]PGD - Batches:   2%|▏         | 20/814 [00:32<21:34,  1.63s/it]PGD - Batches:   3%|▎         | 21/814 [00:34<21:29,  1.63s/it]                                                               Traceback (most recent call last):
  File "tasklauncher-20220111.py", line 223, in <module>
    target_classifier.rmt(args,cle_w_train,cle_y_train, cle_train_dataloader, cle_x_test,cle_y_test,adv_x_test,adv_y_test,exp_result_dir,stylegan2ada_config_kwargs)
  File "/home/maggie/mmat/clamodels/classifier.py", line 1479, in rmt
    epoch_x_test_adv, epoch_y_test_adv = epoch_attack_classifier.generateadvfromtestsettensor(self._cle_test_tensorset_x, self._cle_test_tensorset_y) 
  File "/home/maggie/mmat/attacks/advattack.py", line 408, in generateadvfromtestsettensor
    self._x_test_adv = self._advgenmodel.generate(x = self._x_test, y = self._y_test)
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/art/attacks/attack.py", line 74, in replacement_function
    return fdict[func_name](self, *args, **kwargs)
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent.py", line 181, in generate
    return self._attack.generate(x=x, y=y, **kwargs)
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/art/attacks/attack.py", line 74, in replacement_function
    return fdict[func_name](self, *args, **kwargs)
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py", line 196, in generate
    x=batch, targets=batch_labels, mask=mask_batch, eps=batch_eps, eps_step=batch_eps_step
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py", line 252, in _generate_batch
    adv_x, inputs, targets, mask, eps, eps_step, self.num_random_init > 0 and i_max_iter == 0,
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py", line 385, in _compute_torch
    perturbation = self._compute_perturbation(x_adv, y, mask)
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/art/attacks/evasion/projected_gradient_descent/projected_gradient_descent_pytorch.py", line 279, in _compute_perturbation
    grad = self.estimator.loss_gradient(x=x, y=y) * (1 - 2 * int(self.targeted))
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/art/estimators/classification/pytorch.py", line 1178, in loss_gradient
    loss.backward()
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/tensor.py", line 245, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/autograd/__init__.py", line 147, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
KeyboardInterrupt


---------------------------------------
Torch cuda is available
args.subcommand=run, run the command line
date: 20220111
blackbox attack
Experiment result save dir: /home/maggie/mmat/result/defense/rmt/pgd/basemixup-betasampler/resnet18-svhn/blackbox/20220111/00000
initilize the dataset loading parameters
Using downloaded and verified file: /home/data/maggie/svhn/train_32x32.mat
Loading *svhn* train dataloader finished !
Loading *svhn* test dataloader finished !
initlize classifier
learned calssify model != None
args.adv_dataset： /home/maggie/mmat/result/attack/pgd/resnet18-svhn/20210914/00000-attackacc-0.166756/attack-svhn-dataset/samples
Accuary of before rmt trained classifier on clean testset:93.3774%
Loss of before mmat trained classifier clean testset:0.25881198048591614
args.mix_mode: basemixup
args.mix_w_num: 2
args.beta_alpha: 0.5
args.dirichlet_gama: 1
cle_w_train.shape: torch.Size([24004, 8, 512])
cle_y_train.shape: torch.Size([24004, 8])
cle_x_test.shape: torch.Size([26032, 3, 32, 32])
cle_y_test.shape: torch.Size([26032])
adv_x_test.shape: torch.Size([26032, 3, 32, 32])
adv_y_test.shape: torch.Size([26032])
cle_train_dataloader.len: 287
Accuary of before rmt trained classifier on adversarial testset:16.6756%
Loss of before mmat trained classifier on adversarial testset:15.307662010192871
w_trainset_len: 24004
batch_size: 256
w_batch_num: 94


0epoch learning rate:0.001
Setting up PyTorch plugin "bias_act_plugin"... /home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/utils/cpp_extension.py:286: UserWarning: 

                               !! WARNING !!

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Your compiler (c++) is not compatible with the compiler Pytorch was
built with for this platform, which is g++ on linux. Please
use g++ to to compile your extension. Alternatively, you may
compile PyTorch from source using c++, and then you can also use
c++ to compile your extension.

See https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md for help
with compiling PyTorch from source.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

                              !! WARNING !!

  platform=sys.platform))
Done.
Setting up PyTorch plugin "upfirdn2d_plugin"... /home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/utils/cpp_extension.py:286: UserWarning: 

                               !! WARNING !!

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Your compiler (c++) is not compatible with the compiler Pytorch was
built with for this platform, which is g++ on linux. Please
use g++ to to compile your extension. Alternatively, you may
compile PyTorch from source using c++, and then you can also use
c++ to compile your extension.

See https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md for help
with compiling PyTorch from source.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

                              !! WARNING !!

  platform=sys.platform))
Done.
[Epoch 1/40] [Batch 1/287] [Batch classify loss: 1.625642]
[Epoch 1/40] [Batch 2/287] [Batch classify loss: 1.273485]
[Epoch 1/40] [Batch 3/287] [Batch classify loss: 1.073292]
[Epoch 1/40] [Batch 4/287] [Batch classify loss: 1.151039]
[Epoch 1/40] [Batch 5/287] [Batch classify loss: 1.124246]
[Epoch 1/40] [Batch 6/287] [Batch classify loss: 0.918579]
[Epoch 1/40] [Batch 7/287] [Batch classify loss: 0.975698]
[Epoch 1/40] [Batch 8/287] [Batch classify loss: 0.916203]
[Epoch 1/40] [Batch 9/287] [Batch classify loss: 1.082100]
[Epoch 1/40] [Batch 10/287] [Batch classify loss: 0.976364]
[Epoch 1/40] [Batch 11/287] [Batch classify loss: 0.939972]
[Epoch 1/40] [Batch 12/287] [Batch classify loss: 0.896311]
[Epoch 1/40] [Batch 13/287] [Batch classify loss: 0.928589]
[Epoch 1/40] [Batch 14/287] [Batch classify loss: 0.944323]
[Epoch 1/40] [Batch 15/287] [Batch classify loss: 0.900514]
[Epoch 1/40] [Batch 16/287] [Batch classify loss: 0.828158]
[Epoch 1/40] [Batch 17/287] [Batch classify loss: 0.870876]
[Epoch 1/40] [Batch 18/287] [Batch classify loss: 0.869385]
[Epoch 1/40] [Batch 19/287] [Batch classify loss: 0.842345]
[Epoch 1/40] [Batch 20/287] [Batch classify loss: 0.846043]
[Epoch 1/40] [Batch 21/287] [Batch classify loss: 0.829429]
[Epoch 1/40] [Batch 22/287] [Batch classify loss: 0.872784]
[Epoch 1/40] [Batch 23/287] [Batch classify loss: 0.922510]
[Epoch 1/40] [Batch 24/287] [Batch classify loss: 0.839267]
[Epoch 1/40] [Batch 25/287] [Batch classify loss: 0.834913]
[Epoch 1/40] [Batch 26/287] [Batch classify loss: 0.889030]
[Epoch 1/40] [Batch 27/287] [Batch classify loss: 0.857194]
[Epoch 1/40] [Batch 28/287] [Batch classify loss: 0.862940]
[Epoch 1/40] [Batch 29/287] [Batch classify loss: 0.768167]
[Epoch 1/40] [Batch 30/287] [Batch classify loss: 0.844002]
[Epoch 1/40] [Batch 31/287] [Batch classify loss: 0.826816]
[Epoch 1/40] [Batch 32/287] [Batch classify loss: 0.892570]
[Epoch 1/40] [Batch 33/287] [Batch classify loss: 0.865154]
[Epoch 1/40] [Batch 34/287] [Batch classify loss: 0.938522]
[Epoch 1/40] [Batch 35/287] [Batch classify loss: 0.798853]
[Epoch 1/40] [Batch 36/287] [Batch classify loss: 0.861148]
[Epoch 1/40] [Batch 37/287] [Batch classify loss: 0.886085]
[Epoch 1/40] [Batch 38/287] [Batch classify loss: 0.820521]
[Epoch 1/40] [Batch 39/287] [Batch classify loss: 0.848217]
[Epoch 1/40] [Batch 40/287] [Batch classify loss: 0.915374]
[Epoch 1/40] [Batch 41/287] [Batch classify loss: 0.858057]
[Epoch 1/40] [Batch 42/287] [Batch classify loss: 0.755665]
[Epoch 1/40] [Batch 43/287] [Batch classify loss: 0.844599]
[Epoch 1/40] [Batch 44/287] [Batch classify loss: 0.839385]
[Epoch 1/40] [Batch 45/287] [Batch classify loss: 0.817843]
[Epoch 1/40] [Batch 46/287] [Batch classify loss: 0.820272]
[Epoch 1/40] [Batch 47/287] [Batch classify loss: 0.882440]
[Epoch 1/40] [Batch 48/287] [Batch classify loss: 0.793892]
[Epoch 1/40] [Batch 49/287] [Batch classify loss: 0.831827]
[Epoch 1/40] [Batch 50/287] [Batch classify loss: 0.841878]
[Epoch 1/40] [Batch 51/287] [Batch classify loss: 0.782707]
[Epoch 1/40] [Batch 52/287] [Batch classify loss: 0.779136]
[Epoch 1/40] [Batch 53/287] [Batch classify loss: 0.801956]
[Epoch 1/40] [Batch 54/287] [Batch classify loss: 0.886013]
[Epoch 1/40] [Batch 55/287] [Batch classify loss: 0.817503]
[Epoch 1/40] [Batch 56/287] [Batch classify loss: 0.837098]
[Epoch 1/40] [Batch 57/287] [Batch classify loss: 0.902852]
[Epoch 1/40] [Batch 58/287] [Batch classify loss: 0.906073]
[Epoch 1/40] [Batch 59/287] [Batch classify loss: 0.870902]
[Epoch 1/40] [Batch 60/287] [Batch classify loss: 0.820603]
[Epoch 1/40] [Batch 61/287] [Batch classify loss: 0.774547]
[Epoch 1/40] [Batch 62/287] [Batch classify loss: 0.870506]
[Epoch 1/40] [Batch 63/287] [Batch classify loss: 0.798490]
[Epoch 1/40] [Batch 64/287] [Batch classify loss: 0.902039]
[Epoch 1/40] [Batch 65/287] [Batch classify loss: 0.831519]
[Epoch 1/40] [Batch 66/287] [Batch classify loss: 0.813696]
[Epoch 1/40] [Batch 67/287] [Batch classify loss: 0.825384]
[Epoch 1/40] [Batch 68/287] [Batch classify loss: 0.920778]
[Epoch 1/40] [Batch 69/287] [Batch classify loss: 0.845455]
[Epoch 1/40] [Batch 70/287] [Batch classify loss: 0.762543]
[Epoch 1/40] [Batch 71/287] [Batch classify loss: 0.826575]
[Epoch 1/40] [Batch 72/287] [Batch classify loss: 0.837555]
[Epoch 1/40] [Batch 73/287] [Batch classify loss: 0.780195]
[Epoch 1/40] [Batch 74/287] [Batch classify loss: 0.874676]
[Epoch 1/40] [Batch 75/287] [Batch classify loss: 0.818584]
[Epoch 1/40] [Batch 76/287] [Batch classify loss: 0.828120]
[Epoch 1/40] [Batch 77/287] [Batch classify loss: 0.846227]
[Epoch 1/40] [Batch 78/287] [Batch classify loss: 0.818449]
[Epoch 1/40] [Batch 79/287] [Batch classify loss: 0.833953]
[Epoch 1/40] [Batch 80/287] [Batch classify loss: 0.773065]
[Epoch 1/40] [Batch 81/287] [Batch classify loss: 0.784679]
[Epoch 1/40] [Batch 82/287] [Batch classify loss: 0.845773]
[Epoch 1/40] [Batch 83/287] [Batch classify loss: 0.827657]
[Epoch 1/40] [Batch 84/287] [Batch classify loss: 0.842624]
[Epoch 1/40] [Batch 85/287] [Batch classify loss: 0.821807]
[Epoch 1/40] [Batch 86/287] [Batch classify loss: 0.813680]
[Epoch 1/40] [Batch 87/287] [Batch classify loss: 0.867097]
[Epoch 1/40] [Batch 88/287] [Batch classify loss: 0.772856]
[Epoch 1/40] [Batch 89/287] [Batch classify loss: 0.868112]
[Epoch 1/40] [Batch 90/287] [Batch classify loss: 0.849385]
[Epoch 1/40] [Batch 91/287] [Batch classify loss: 0.811524]
[Epoch 1/40] [Batch 92/287] [Batch classify loss: 0.850513]
[Epoch 1/40] [Batch 93/287] [Batch classify loss: 0.835757]
[Epoch 1/40] [Batch 94/287] [Batch classify loss: 0.749887]
[Epoch 1/40] [Batch 95/287] [Batch classify loss: 0.798854]
[Epoch 1/40] [Batch 96/287] [Batch classify loss: 0.782930]
[Epoch 1/40] [Batch 97/287] [Batch classify loss: 0.808871]
[Epoch 1/40] [Batch 98/287] [Batch classify loss: 0.765605]
[Epoch 1/40] [Batch 99/287] [Batch classify loss: 0.835463]
[Epoch 1/40] [Batch 100/287] [Batch classify loss: 0.758498]
[Epoch 1/40] [Batch 101/287] [Batch classify loss: 0.793350]
[Epoch 1/40] [Batch 102/287] [Batch classify loss: 0.848215]
[Epoch 1/40] [Batch 103/287] [Batch classify loss: 0.869830]
[Epoch 1/40] [Batch 104/287] [Batch classify loss: 0.798863]
[Epoch 1/40] [Batch 105/287] [Batch classify loss: 0.787183]
[Epoch 1/40] [Batch 106/287] [Batch classify loss: 0.804612]
[Epoch 1/40] [Batch 107/287] [Batch classify loss: 0.730383]
[Epoch 1/40] [Batch 108/287] [Batch classify loss: 0.847024]
[Epoch 1/40] [Batch 109/287] [Batch classify loss: 0.790954]
[Epoch 1/40] [Batch 110/287] [Batch classify loss: 0.845616]
[Epoch 1/40] [Batch 111/287] [Batch classify loss: 0.784223]
[Epoch 1/40] [Batch 112/287] [Batch classify loss: 0.826284]
[Epoch 1/40] [Batch 113/287] [Batch classify loss: 0.765170]
[Epoch 1/40] [Batch 114/287] [Batch classify loss: 0.846818]
[Epoch 1/40] [Batch 115/287] [Batch classify loss: 0.664281]
[Epoch 1/40] [Batch 116/287] [Batch classify loss: 0.778936]
[Epoch 1/40] [Batch 117/287] [Batch classify loss: 0.761617]
[Epoch 1/40] [Batch 118/287] [Batch classify loss: 0.772665]
[Epoch 1/40] [Batch 119/287] [Batch classify loss: 0.749796]
[Epoch 1/40] [Batch 120/287] [Batch classify loss: 0.823986]
[Epoch 1/40] [Batch 121/287] [Batch classify loss: 0.774800]
[Epoch 1/40] [Batch 122/287] [Batch classify loss: 0.756710]
[Epoch 1/40] [Batch 123/287] [Batch classify loss: 0.818566]
[Epoch 1/40] [Batch 124/287] [Batch classify loss: 0.892457]
[Epoch 1/40] [Batch 125/287] [Batch classify loss: 0.834337]
[Epoch 1/40] [Batch 126/287] [Batch classify loss: 0.838956]
[Epoch 1/40] [Batch 127/287] [Batch classify loss: 0.805763]
[Epoch 1/40] [Batch 128/287] [Batch classify loss: 0.804120]
[Epoch 1/40] [Batch 129/287] [Batch classify loss: 0.839859]
[Epoch 1/40] [Batch 130/287] [Batch classify loss: 0.862511]
[Epoch 1/40] [Batch 131/287] [Batch classify loss: 0.825553]
[Epoch 1/40] [Batch 132/287] [Batch classify loss: 0.795747]
[Epoch 1/40] [Batch 133/287] [Batch classify loss: 0.784290]
[Epoch 1/40] [Batch 134/287] [Batch classify loss: 0.904750]
[Epoch 1/40] [Batch 135/287] [Batch classify loss: 0.785193]
[Epoch 1/40] [Batch 136/287] [Batch classify loss: 0.784186]
[Epoch 1/40] [Batch 137/287] [Batch classify loss: 0.780705]
[Epoch 1/40] [Batch 138/287] [Batch classify loss: 0.747247]
[Epoch 1/40] [Batch 139/287] [Batch classify loss: 0.792746]
[Epoch 1/40] [Batch 140/287] [Batch classify loss: 0.799753]
[Epoch 1/40] [Batch 141/287] [Batch classify loss: 0.865753]
[Epoch 1/40] [Batch 142/287] [Batch classify loss: 0.783247]
[Epoch 1/40] [Batch 143/287] [Batch classify loss: 0.777750]
[Epoch 1/40] [Batch 144/287] [Batch classify loss: 0.820307]
[Epoch 1/40] [Batch 145/287] [Batch classify loss: 0.763606]
[Epoch 1/40] [Batch 146/287] [Batch classify loss: 0.763724]
[Epoch 1/40] [Batch 147/287] [Batch classify loss: 0.821545]
[Epoch 1/40] [Batch 148/287] [Batch classify loss: 0.819582]
[Epoch 1/40] [Batch 149/287] [Batch classify loss: 0.863225]
[Epoch 1/40] [Batch 150/287] [Batch classify loss: 0.828361]
[Epoch 1/40] [Batch 151/287] [Batch classify loss: 0.908568]
[Epoch 1/40] [Batch 152/287] [Batch classify loss: 0.737688]
[Epoch 1/40] [Batch 153/287] [Batch classify loss: 0.821859]
[Epoch 1/40] [Batch 154/287] [Batch classify loss: 0.787783]
[Epoch 1/40] [Batch 155/287] [Batch classify loss: 0.771985]
[Epoch 1/40] [Batch 156/287] [Batch classify loss: 0.822818]
[Epoch 1/40] [Batch 157/287] [Batch classify loss: 0.767430]
[Epoch 1/40] [Batch 158/287] [Batch classify loss: 0.886056]
[Epoch 1/40] [Batch 159/287] [Batch classify loss: 0.765202]
[Epoch 1/40] [Batch 160/287] [Batch classify loss: 0.890290]
[Epoch 1/40] [Batch 161/287] [Batch classify loss: 0.803093]
[Epoch 1/40] [Batch 162/287] [Batch classify loss: 0.901430]
[Epoch 1/40] [Batch 163/287] [Batch classify loss: 0.799502]
[Epoch 1/40] [Batch 164/287] [Batch classify loss: 0.777555]
[Epoch 1/40] [Batch 165/287] [Batch classify loss: 0.828862]
[Epoch 1/40] [Batch 166/287] [Batch classify loss: 0.757534]
[Epoch 1/40] [Batch 167/287] [Batch classify loss: 0.767502]
[Epoch 1/40] [Batch 168/287] [Batch classify loss: 0.823251]
[Epoch 1/40] [Batch 169/287] [Batch classify loss: 0.864139]
[Epoch 1/40] [Batch 170/287] [Batch classify loss: 0.802272]
[Epoch 1/40] [Batch 171/287] [Batch classify loss: 0.786625]
[Epoch 1/40] [Batch 172/287] [Batch classify loss: 0.698174]
[Epoch 1/40] [Batch 173/287] [Batch classify loss: 0.751743]
[Epoch 1/40] [Batch 174/287] [Batch classify loss: 0.823456]
[Epoch 1/40] [Batch 175/287] [Batch classify loss: 0.802143]
[Epoch 1/40] [Batch 176/287] [Batch classify loss: 0.832669]
[Epoch 1/40] [Batch 177/287] [Batch classify loss: 0.772341]
[Epoch 1/40] [Batch 178/287] [Batch classify loss: 0.757684]
[Epoch 1/40] [Batch 179/287] [Batch classify loss: 0.749080]
[Epoch 1/40] [Batch 180/287] [Batch classify loss: 0.792040]
[Epoch 1/40] [Batch 181/287] [Batch classify loss: 0.797903]
[Epoch 1/40] [Batch 182/287] [Batch classify loss: 0.890314]
[Epoch 1/40] [Batch 183/287] [Batch classify loss: 0.711120]
[Epoch 1/40] [Batch 184/287] [Batch classify loss: 0.940381]
[Epoch 1/40] [Batch 185/287] [Batch classify loss: 0.796185]
[Epoch 1/40] [Batch 186/287] [Batch classify loss: 0.814902]
[Epoch 1/40] [Batch 187/287] [Batch classify loss: 0.781222]
[Epoch 1/40] [Batch 188/287] [Batch classify loss: 0.775085]
[Epoch 1/40] [Batch 189/287] [Batch classify loss: 0.752495]
[Epoch 1/40] [Batch 190/287] [Batch classify loss: 0.731736]
[Epoch 1/40] [Batch 191/287] [Batch classify loss: 0.822497]
[Epoch 1/40] [Batch 192/287] [Batch classify loss: 0.839544]
[Epoch 1/40] [Batch 193/287] [Batch classify loss: 0.797932]
[Epoch 1/40] [Batch 194/287] [Batch classify loss: 0.691717]
[Epoch 1/40] [Batch 195/287] [Batch classify loss: 0.736810]
[Epoch 1/40] [Batch 196/287] [Batch classify loss: 0.741194]
[Epoch 1/40] [Batch 197/287] [Batch classify loss: 0.794636]
[Epoch 1/40] [Batch 198/287] [Batch classify loss: 0.763583]
[Epoch 1/40] [Batch 199/287] [Batch classify loss: 0.780461]
[Epoch 1/40] [Batch 200/287] [Batch classify loss: 0.768567]
[Epoch 1/40] [Batch 201/287] [Batch classify loss: 0.768875]
[Epoch 1/40] [Batch 202/287] [Batch classify loss: 0.825217]
[Epoch 1/40] [Batch 203/287] [Batch classify loss: 0.812741]
[Epoch 1/40] [Batch 204/287] [Batch classify loss: 0.772334]
[Epoch 1/40] [Batch 205/287] [Batch classify loss: 0.801647]
[Epoch 1/40] [Batch 206/287] [Batch classify loss: 0.782144]
[Epoch 1/40] [Batch 207/287] [Batch classify loss: 0.712423]
[Epoch 1/40] [Batch 208/287] [Batch classify loss: 0.731012]
[Epoch 1/40] [Batch 209/287] [Batch classify loss: 0.727955]
[Epoch 1/40] [Batch 210/287] [Batch classify loss: 0.804137]
[Epoch 1/40] [Batch 211/287] [Batch classify loss: 0.861235]
[Epoch 1/40] [Batch 212/287] [Batch classify loss: 0.846931]
[Epoch 1/40] [Batch 213/287] [Batch classify loss: 0.807699]
[Epoch 1/40] [Batch 214/287] [Batch classify loss: 0.781627]
[Epoch 1/40] [Batch 215/287] [Batch classify loss: 0.817712]
[Epoch 1/40] [Batch 216/287] [Batch classify loss: 0.796854]
[Epoch 1/40] [Batch 217/287] [Batch classify loss: 0.762030]
[Epoch 1/40] [Batch 218/287] [Batch classify loss: 0.798944]
[Epoch 1/40] [Batch 219/287] [Batch classify loss: 0.730936]
[Epoch 1/40] [Batch 220/287] [Batch classify loss: 0.830817]
[Epoch 1/40] [Batch 221/287] [Batch classify loss: 0.782999]
[Epoch 1/40] [Batch 222/287] [Batch classify loss: 0.772021]
[Epoch 1/40] [Batch 223/287] [Batch classify loss: 0.869398]
[Epoch 1/40] [Batch 224/287] [Batch classify loss: 0.843210]
[Epoch 1/40] [Batch 225/287] [Batch classify loss: 0.760889]
[Epoch 1/40] [Batch 226/287] [Batch classify loss: 0.776997]
[Epoch 1/40] [Batch 227/287] [Batch classify loss: 0.861315]
[Epoch 1/40] [Batch 228/287] [Batch classify loss: 0.749819]
[Epoch 1/40] [Batch 229/287] [Batch classify loss: 0.753121]
[Epoch 1/40] [Batch 230/287] [Batch classify loss: 0.805625]
[Epoch 1/40] [Batch 231/287] [Batch classify loss: 0.859114]
[Epoch 1/40] [Batch 232/287] [Batch classify loss: 0.753418]
[Epoch 1/40] [Batch 233/287] [Batch classify loss: 0.748079]
[Epoch 1/40] [Batch 234/287] [Batch classify loss: 0.757165]
[Epoch 1/40] [Batch 235/287] [Batch classify loss: 0.827275]
[Epoch 1/40] [Batch 236/287] [Batch classify loss: 0.794773]
Traceback (most recent call last):
  File "tasklauncher-20220111.py", line 223, in <module>
    target_classifier.rmt(args,cle_w_train,cle_y_train, cle_train_dataloader, cle_x_test,cle_y_test,adv_x_test,adv_y_test,exp_result_dir,stylegan2ada_config_kwargs)
  File "/home/maggie/mmat/clamodels/classifier.py", line 1574, in rmt
    print("[Epoch %d/%d] [Batch %d/%d] [Batch classify loss: %f]" % (epoch_index+1, self._args.epochs, batch_index+1, len(self._train_dataloader), loss.item()))
KeyboardInterrupt
