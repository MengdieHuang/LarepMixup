

---------------------------------------
Torch cuda is available
args.subcommand=run, run the command line
date: 20220104
whitebox attack
Experiment result save dir: /home/maggie/mmat/result/defense/inputmixup/fgsm/basemixup-betasampler/preactresnet34-cifar10/whitebox/20220104/00000
initilize the dataset loading parameters
load cifar10 dataset
Loading *cifar10* train dataloader finished !
Loading *cifar10* test dataloader finished !
initlize classifier
learned calssify model != None
cle_x_train.shape: torch.Size([25397, 3, 32, 32])
cle_y_train.shape: torch.Size([25397])
cle_y_train.shape: torch.Size([25397, 10])
args.adv_dataset： /home/maggie/mmat/result/attack/fgsm/preactresnet34-cifar10/20210927/00000-attackacc-18.04/attack-cifar10-dataset/samples
Accuary of before rmt trained classifier on clean testset:83.5700%
Loss of before mmat trained classifier clean testset:0.6700245141983032
Accuary of before rmt trained classifier on white-box adv testset:18.0400%
Loss of before rmt trained classifier on white-box adv testset:5.7243146896362305
args.mix_mode: basemixup
args.mix_w_num: 2
args.beta_alpha: 0.5
args.dirichlet_gama: 1
compare with---------input mixup train--------------
cle_x_train.shape: torch.Size([25397, 3, 32, 32])
cle_y_train.shape: torch.Size([25397, 10])
cle_x_test.shape: torch.Size([10000, 3, 32, 32])
cle_y_test.shape: torch.Size([10000])
adv_x_test.shape: torch.Size([10000, 3, 32, 32])
adv_y_test.shape: torch.Size([10000])
白盒
initlize attack classifier
generate pixel adversarial exampels
Get FGSM examples generate model
self._args.attack_eps: 0.3
generating testset adversarial examples...
finished generate testset adversarial examples !
Accuary of before rmt trained classifier on adversarial testset:18.0400%
Loss of before mmat trained classifier on adversarial testset:5.724340915679932
w_trainset_len: 25397
batch_size: 256
w_batch_num: 100


0epoch learning rate:0.001
[Epoch 1/40] [Batch 1/196] [Batch classify loss: 1.081697]
[Epoch 1/40] [Batch 2/196] [Batch classify loss: 1.497413]
[Epoch 1/40] [Batch 3/196] [Batch classify loss: 0.994525]
[Epoch 1/40] [Batch 4/196] [Batch classify loss: 1.146690]
[Epoch 1/40] [Batch 5/196] [Batch classify loss: 0.812291]
[Epoch 1/40] [Batch 6/196] [Batch classify loss: 0.855349]
[Epoch 1/40] [Batch 7/196] [Batch classify loss: 1.351410]
[Epoch 1/40] [Batch 8/196] [Batch classify loss: 0.677321]
[Epoch 1/40] [Batch 9/196] [Batch classify loss: 1.122007]
[Epoch 1/40] [Batch 10/196] [Batch classify loss: 0.775107]
[Epoch 1/40] [Batch 11/196] [Batch classify loss: 0.553896]
[Epoch 1/40] [Batch 12/196] [Batch classify loss: 1.228569]
[Epoch 1/40] [Batch 13/196] [Batch classify loss: 0.862119]
[Epoch 1/40] [Batch 14/196] [Batch classify loss: 0.706190]
[Epoch 1/40] [Batch 15/196] [Batch classify loss: 0.724697]
[Epoch 1/40] [Batch 16/196] [Batch classify loss: 1.328362]
[Epoch 1/40] [Batch 17/196] [Batch classify loss: 0.722226]
[Epoch 1/40] [Batch 18/196] [Batch classify loss: 0.516284]
[Epoch 1/40] [Batch 19/196] [Batch classify loss: 0.563846]
[Epoch 1/40] [Batch 20/196] [Batch classify loss: 0.743732]
[Epoch 1/40] [Batch 21/196] [Batch classify loss: 1.355169]
[Epoch 1/40] [Batch 22/196] [Batch classify loss: 1.215583]
[Epoch 1/40] [Batch 23/196] [Batch classify loss: 0.876288]
[Epoch 1/40] [Batch 24/196] [Batch classify loss: 0.639049]
[Epoch 1/40] [Batch 25/196] [Batch classify loss: 0.477716]
[Epoch 1/40] [Batch 26/196] [Batch classify loss: 1.236119]
[Epoch 1/40] [Batch 27/196] [Batch classify loss: 1.156856]
[Epoch 1/40] [Batch 28/196] [Batch classify loss: 0.726547]
[Epoch 1/40] [Batch 29/196] [Batch classify loss: 0.651419]
[Epoch 1/40] [Batch 30/196] [Batch classify loss: 0.478649]
[Epoch 1/40] [Batch 31/196] [Batch classify loss: 0.429801]
[Epoch 1/40] [Batch 32/196] [Batch classify loss: 0.735442]
[Epoch 1/40] [Batch 33/196] [Batch classify loss: 0.753548]
[Epoch 1/40] [Batch 34/196] [Batch classify loss: 1.400854]
[Epoch 1/40] [Batch 35/196] [Batch classify loss: 0.752443]
[Epoch 1/40] [Batch 36/196] [Batch classify loss: 1.013134]
[Epoch 1/40] [Batch 37/196] [Batch classify loss: 0.475853]
[Epoch 1/40] [Batch 38/196] [Batch classify loss: 0.751354]
[Epoch 1/40] [Batch 39/196] [Batch classify loss: 1.046228]
[Epoch 1/40] [Batch 40/196] [Batch classify loss: 0.696142]
[Epoch 1/40] [Batch 41/196] [Batch classify loss: 0.951008]
[Epoch 1/40] [Batch 42/196] [Batch classify loss: 0.658521]
[Epoch 1/40] [Batch 43/196] [Batch classify loss: 0.505608]
[Epoch 1/40] [Batch 44/196] [Batch classify loss: 0.843096]
[Epoch 1/40] [Batch 45/196] [Batch classify loss: 0.986607]
[Epoch 1/40] [Batch 46/196] [Batch classify loss: 1.260897]
[Epoch 1/40] [Batch 47/196] [Batch classify loss: 0.510305]
[Epoch 1/40] [Batch 48/196] [Batch classify loss: 0.470103]
[Epoch 1/40] [Batch 49/196] [Batch classify loss: 0.434870]
[Epoch 1/40] [Batch 50/196] [Batch classify loss: 0.380785]
[Epoch 1/40] [Batch 51/196] [Batch classify loss: 1.305608]
[Epoch 1/40] [Batch 52/196] [Batch classify loss: 1.347661]
[Epoch 1/40] [Batch 53/196] [Batch classify loss: 0.491514]
[Epoch 1/40] [Batch 54/196] [Batch classify loss: 1.189610]
[Epoch 1/40] [Batch 55/196] [Batch classify loss: 1.104433]
[Epoch 1/40] [Batch 56/196] [Batch classify loss: 1.129526]
[Epoch 1/40] [Batch 57/196] [Batch classify loss: 0.947896]
[Epoch 1/40] [Batch 58/196] [Batch classify loss: 1.156184]
[Epoch 1/40] [Batch 59/196] [Batch classify loss: 0.640146]
[Epoch 1/40] [Batch 60/196] [Batch classify loss: 1.111917]
[Epoch 1/40] [Batch 61/196] [Batch classify loss: 0.599177]
[Epoch 1/40] [Batch 62/196] [Batch classify loss: 0.557872]
[Epoch 1/40] [Batch 63/196] [Batch classify loss: 0.493211]
[Epoch 1/40] [Batch 64/196] [Batch classify loss: 0.411792]
[Epoch 1/40] [Batch 65/196] [Batch classify loss: 0.644548]
[Epoch 1/40] [Batch 66/196] [Batch classify loss: 1.398765]
[Epoch 1/40] [Batch 67/196] [Batch classify loss: 1.467373]
[Epoch 1/40] [Batch 68/196] [Batch classify loss: 1.002785]
[Epoch 1/40] [Batch 69/196] [Batch classify loss: 1.054584]
[Epoch 1/40] [Batch 70/196] [Batch classify loss: 0.582634]
[Epoch 1/40] [Batch 71/196] [Batch classify loss: 0.816088]
[Epoch 1/40] [Batch 72/196] [Batch classify loss: 0.907333]
[Epoch 1/40] [Batch 73/196] [Batch classify loss: 0.881394]
[Epoch 1/40] [Batch 74/196] [Batch classify loss: 0.614195]
[Epoch 1/40] [Batch 75/196] [Batch classify loss: 0.786638]
[Epoch 1/40] [Batch 76/196] [Batch classify loss: 1.174889]
[Epoch 1/40] [Batch 77/196] [Batch classify loss: 0.579297]
[Epoch 1/40] [Batch 78/196] [Batch classify loss: 1.224199]
[Epoch 1/40] [Batch 79/196] [Batch classify loss: 0.525080]
[Epoch 1/40] [Batch 80/196] [Batch classify loss: 1.155903]
[Epoch 1/40] [Batch 81/196] [Batch classify loss: 1.308964]
[Epoch 1/40] [Batch 82/196] [Batch classify loss: 0.665359]
[Epoch 1/40] [Batch 83/196] [Batch classify loss: 0.661137]
[Epoch 1/40] [Batch 84/196] [Batch classify loss: 1.030063]
[Epoch 1/40] [Batch 85/196] [Batch classify loss: 1.192112]
[Epoch 1/40] [Batch 86/196] [Batch classify loss: 0.944302]
[Epoch 1/40] [Batch 87/196] [Batch classify loss: 0.610148]
[Epoch 1/40] [Batch 88/196] [Batch classify loss: 1.250740]
[Epoch 1/40] [Batch 89/196] [Batch classify loss: 0.992358]
[Epoch 1/40] [Batch 90/196] [Batch classify loss: 1.202615]
[Epoch 1/40] [Batch 91/196] [Batch classify loss: 0.971298]
[Epoch 1/40] [Batch 92/196] [Batch classify loss: 1.254285]
[Epoch 1/40] [Batch 93/196] [Batch classify loss: 1.046123]
[Epoch 1/40] [Batch 94/196] [Batch classify loss: 1.254312]
[Epoch 1/40] [Batch 95/196] [Batch classify loss: 0.637682]
[Epoch 1/40] [Batch 96/196] [Batch classify loss: 0.601593]
[Epoch 1/40] [Batch 97/196] [Batch classify loss: 0.467979]
[Epoch 1/40] [Batch 98/196] [Batch classify loss: 0.459038]
[Epoch 1/40] [Batch 99/196] [Batch classify loss: 1.307405]
[Epoch 1/40] [Batch 100/196] [Batch classify loss: 0.665912]
[Epoch 1/40] [Batch 101/196] [Batch classify loss: 0.429593]
[Epoch 1/40] [Batch 102/196] [Batch classify loss: 1.163359]
[Epoch 1/40] [Batch 103/196] [Batch classify loss: 1.083706]
[Epoch 1/40] [Batch 104/196] [Batch classify loss: 1.240743]
[Epoch 1/40] [Batch 105/196] [Batch classify loss: 0.505977]
[Epoch 1/40] [Batch 106/196] [Batch classify loss: 0.859988]
[Epoch 1/40] [Batch 107/196] [Batch classify loss: 0.566863]
[Epoch 1/40] [Batch 108/196] [Batch classify loss: 0.710713]
[Epoch 1/40] [Batch 109/196] [Batch classify loss: 0.509241]
[Epoch 1/40] [Batch 110/196] [Batch classify loss: 1.220304]
[Epoch 1/40] [Batch 111/196] [Batch classify loss: 1.222867]
[Epoch 1/40] [Batch 112/196] [Batch classify loss: 0.639405]
[Epoch 1/40] [Batch 113/196] [Batch classify loss: 0.425470]
[Epoch 1/40] [Batch 114/196] [Batch classify loss: 0.392947]
[Epoch 1/40] [Batch 115/196] [Batch classify loss: 0.549489]
[Epoch 1/40] [Batch 116/196] [Batch classify loss: 0.465609]
[Epoch 1/40] [Batch 117/196] [Batch classify loss: 0.823141]
[Epoch 1/40] [Batch 118/196] [Batch classify loss: 1.071613]
[Epoch 1/40] [Batch 119/196] [Batch classify loss: 0.991439]
[Epoch 1/40] [Batch 120/196] [Batch classify loss: 1.220079]
[Epoch 1/40] [Batch 121/196] [Batch classify loss: 0.366520]
[Epoch 1/40] [Batch 122/196] [Batch classify loss: 0.536723]
[Epoch 1/40] [Batch 123/196] [Batch classify loss: 0.414623]
[Epoch 1/40] [Batch 124/196] [Batch classify loss: 1.031216]
[Epoch 1/40] [Batch 125/196] [Batch classify loss: 0.618845]
[Epoch 1/40] [Batch 126/196] [Batch classify loss: 0.835038]
[Epoch 1/40] [Batch 127/196] [Batch classify loss: 0.461618]
[Epoch 1/40] [Batch 128/196] [Batch classify loss: 0.413657]
[Epoch 1/40] [Batch 129/196] [Batch classify loss: 0.435807]
[Epoch 1/40] [Batch 130/196] [Batch classify loss: 0.433806]
[Epoch 1/40] [Batch 131/196] [Batch classify loss: 1.286832]
[Epoch 1/40] [Batch 132/196] [Batch classify loss: 1.398347]
[Epoch 1/40] [Batch 133/196] [Batch classify loss: 0.801126]
[Epoch 1/40] [Batch 134/196] [Batch classify loss: 1.054381]
[Epoch 1/40] [Batch 135/196] [Batch classify loss: 1.219156]
[Epoch 1/40] [Batch 136/196] [Batch classify loss: 1.130641]
[Epoch 1/40] [Batch 137/196] [Batch classify loss: 0.553524]
[Epoch 1/40] [Batch 138/196] [Batch classify loss: 1.241732]
[Epoch 1/40] [Batch 139/196] [Batch classify loss: 0.572032]
[Epoch 1/40] [Batch 140/196] [Batch classify loss: 0.527838]
[Epoch 1/40] [Batch 141/196] [Batch classify loss: 0.511244]
[Epoch 1/40] [Batch 142/196] [Batch classify loss: 0.486366]
[Epoch 1/40] [Batch 143/196] [Batch classify loss: 1.070455]
[Epoch 1/40] [Batch 144/196] [Batch classify loss: 0.489000]
[Epoch 1/40] [Batch 145/196] [Batch classify loss: 0.307581]
[Epoch 1/40] [Batch 146/196] [Batch classify loss: 0.688017]
[Epoch 1/40] [Batch 147/196] [Batch classify loss: 0.841860]
[Epoch 1/40] [Batch 148/196] [Batch classify loss: 1.349414]
[Epoch 1/40] [Batch 149/196] [Batch classify loss: 1.365422]
[Epoch 1/40] [Batch 150/196] [Batch classify loss: 1.126781]
[Epoch 1/40] [Batch 151/196] [Batch classify loss: 1.031086]
[Epoch 1/40] [Batch 152/196] [Batch classify loss: 0.607744]
[Epoch 1/40] [Batch 153/196] [Batch classify loss: 0.687964]
[Epoch 1/40] [Batch 154/196] [Batch classify loss: 0.504731]
[Epoch 1/40] [Batch 155/196] [Batch classify loss: 0.549469]
[Epoch 1/40] [Batch 156/196] [Batch classify loss: 0.832055]
[Epoch 1/40] [Batch 157/196] [Batch classify loss: 1.213430]
[Epoch 1/40] [Batch 158/196] [Batch classify loss: 0.553427]
[Epoch 1/40] [Batch 159/196] [Batch classify loss: 1.241344]
[Epoch 1/40] [Batch 160/196] [Batch classify loss: 0.568577]
[Epoch 1/40] [Batch 161/196] [Batch classify loss: 1.107059]
[Epoch 1/40] [Batch 162/196] [Batch classify loss: 0.596701]
[Epoch 1/40] [Batch 163/196] [Batch classify loss: 0.543496]
[Epoch 1/40] [Batch 164/196] [Batch classify loss: 0.448143]
[Epoch 1/40] [Batch 165/196] [Batch classify loss: 0.382593]
[Epoch 1/40] [Batch 166/196] [Batch classify loss: 0.791380]
[Epoch 1/40] [Batch 167/196] [Batch classify loss: 0.744632]
[Epoch 1/40] [Batch 168/196] [Batch classify loss: 0.440397]
[Epoch 1/40] [Batch 169/196] [Batch classify loss: 0.671027]
[Epoch 1/40] [Batch 170/196] [Batch classify loss: 0.475338]
[Epoch 1/40] [Batch 171/196] [Batch classify loss: 1.110951]
[Epoch 1/40] [Batch 172/196] [Batch classify loss: 1.199370]
[Epoch 1/40] [Batch 173/196] [Batch classify loss: 0.616777]
[Epoch 1/40] [Batch 174/196] [Batch classify loss: 1.147567]
[Epoch 1/40] [Batch 175/196] [Batch classify loss: 0.875908]
[Epoch 1/40] [Batch 176/196] [Batch classify loss: 0.691286]
[Epoch 1/40] [Batch 177/196] [Batch classify loss: 0.487137]
[Epoch 1/40] [Batch 178/196] [Batch classify loss: 0.654349]
[Epoch 1/40] [Batch 179/196] [Batch classify loss: 1.083351]
[Epoch 1/40] [Batch 180/196] [Batch classify loss: 1.141219]
[Epoch 1/40] [Batch 181/196] [Batch classify loss: 0.932478]
[Epoch 1/40] [Batch 182/196] [Batch classify loss: 0.860639]
[Epoch 1/40] [Batch 183/196] [Batch classify loss: 0.498101]
[Epoch 1/40] [Batch 184/196] [Batch classify loss: 0.489891]
[Epoch 1/40] [Batch 185/196] [Batch classify loss: 0.509978]
[Epoch 1/40] [Batch 186/196] [Batch classify loss: 0.431197]
[Epoch 1/40] [Batch 187/196] [Batch classify loss: 0.565332]
[Epoch 1/40] [Batch 188/196] [Batch classify loss: 1.222350]
[Epoch 1/40] [Batch 189/196] [Batch classify loss: 0.939868]
[Epoch 1/40] [Batch 190/196] [Batch classify loss: 0.422031]
[Epoch 1/40] [Batch 191/196] [Batch classify loss: 0.548042]
[Epoch 1/40] [Batch 192/196] [Batch classify loss: 0.385768]
[Epoch 1/40] [Batch 193/196] [Batch classify loss: 0.996499]
[Epoch 1/40] [Batch 194/196] [Batch classify loss: 0.731422]
[Epoch 1/40] [Batch 195/196] [Batch classify loss: 0.321985]
[Epoch 1/40] [Batch 196/196] [Batch classify loss: 0.685551]
0001 epoch rmt trained classifier accuary on the clean testing examples:79.3300%
0001 epoch rmt trained classifier loss on the clean testing examples:0.6497
initlize attack classifier
generate pixel adversarial exampels
Get FGSM examples generate model
self._args.attack_eps: 0.3
generating testset adversarial examples...
finished generate testset adversarial examples !
0001 epoch rmt trained classifier accuary on adversarial testset:19.9400%
0001 epoch rmt trained classifier loss on adversarial testset:3.2625699043273926


1epoch learning rate:0.001
[Epoch 2/40] [Batch 1/196] [Batch classify loss: 1.133138]
[Epoch 2/40] [Batch 2/196] [Batch classify loss: 0.814293]
[Epoch 2/40] [Batch 3/196] [Batch classify loss: 0.934415]
[Epoch 2/40] [Batch 4/196] [Batch classify loss: 0.629609]
[Epoch 2/40] [Batch 5/196] [Batch classify loss: 0.798138]
[Epoch 2/40] [Batch 6/196] [Batch classify loss: 1.199158]
[Epoch 2/40] [Batch 7/196] [Batch classify loss: 0.638343]
[Epoch 2/40] [Batch 8/196] [Batch classify loss: 1.136026]
[Epoch 2/40] [Batch 9/196] [Batch classify loss: 0.905513]
[Epoch 2/40] [Batch 10/196] [Batch classify loss: 1.118585]
[Epoch 2/40] [Batch 11/196] [Batch classify loss: 0.524779]
[Epoch 2/40] [Batch 12/196] [Batch classify loss: 0.478129]
[Epoch 2/40] [Batch 13/196] [Batch classify loss: 1.167923]
[Epoch 2/40] [Batch 14/196] [Batch classify loss: 1.204600]
[Epoch 2/40] [Batch 15/196] [Batch classify loss: 0.475215]
[Epoch 2/40] [Batch 16/196] [Batch classify loss: 1.152419]
[Epoch 2/40] [Batch 17/196] [Batch classify loss: 0.323670]
[Epoch 2/40] [Batch 18/196] [Batch classify loss: 0.307344]
[Epoch 2/40] [Batch 19/196] [Batch classify loss: 0.286100]
[Epoch 2/40] [Batch 20/196] [Batch classify loss: 1.268500]
[Epoch 2/40] [Batch 21/196] [Batch classify loss: 1.117133]
[Epoch 2/40] [Batch 22/196] [Batch classify loss: 0.291512]
[Epoch 2/40] [Batch 23/196] [Batch classify loss: 0.427078]
[Epoch 2/40] [Batch 24/196] [Batch classify loss: 1.185184]
[Epoch 2/40] [Batch 25/196] [Batch classify loss: 0.406962]
[Epoch 2/40] [Batch 26/196] [Batch classify loss: 0.394530]
[Epoch 2/40] [Batch 27/196] [Batch classify loss: 0.338861]
[Epoch 2/40] [Batch 28/196] [Batch classify loss: 0.560944]
[Epoch 2/40] [Batch 29/196] [Batch classify loss: 0.950225]
[Epoch 2/40] [Batch 30/196] [Batch classify loss: 1.143135]
[Epoch 2/40] [Batch 31/196] [Batch classify loss: 1.153267]
[Epoch 2/40] [Batch 32/196] [Batch classify loss: 0.826692]
[Epoch 2/40] [Batch 33/196] [Batch classify loss: 0.880094]
[Epoch 2/40] [Batch 34/196] [Batch classify loss: 0.932738]
[Epoch 2/40] [Batch 35/196] [Batch classify loss: 0.383367]
[Epoch 2/40] [Batch 36/196] [Batch classify loss: 1.041261]
[Epoch 2/40] [Batch 37/196] [Batch classify loss: 1.070083]
[Epoch 2/40] [Batch 38/196] [Batch classify loss: 0.447871]
[Epoch 2/40] [Batch 39/196] [Batch classify loss: 0.366539]
[Epoch 2/40] [Batch 40/196] [Batch classify loss: 1.155085]
[Epoch 2/40] [Batch 41/196] [Batch classify loss: 0.668400]
[Epoch 2/40] [Batch 42/196] [Batch classify loss: 0.764690]
[Epoch 2/40] [Batch 43/196] [Batch classify loss: 0.809873]
[Epoch 2/40] [Batch 44/196] [Batch classify loss: 1.081255]
[Epoch 2/40] [Batch 45/196] [Batch classify loss: 0.912355]
[Epoch 2/40] [Batch 46/196] [Batch classify loss: 0.372424]
[Epoch 2/40] [Batch 47/196] [Batch classify loss: 0.485568]
[Epoch 2/40] [Batch 48/196] [Batch classify loss: 0.676602]
[Epoch 2/40] [Batch 49/196] [Batch classify loss: 0.726521]
[Epoch 2/40] [Batch 50/196] [Batch classify loss: 0.364744]
[Epoch 2/40] [Batch 51/196] [Batch classify loss: 0.292351]
[Epoch 2/40] [Batch 52/196] [Batch classify loss: 0.982101]
[Epoch 2/40] [Batch 53/196] [Batch classify loss: 0.314422]
[Epoch 2/40] [Batch 54/196] [Batch classify loss: 0.299664]
[Epoch 2/40] [Batch 55/196] [Batch classify loss: 1.233685]
[Epoch 2/40] [Batch 56/196] [Batch classify loss: 0.755766]
[Epoch 2/40] [Batch 57/196] [Batch classify loss: 0.604711]
[Epoch 2/40] [Batch 58/196] [Batch classify loss: 1.258056]
[Epoch 2/40] [Batch 59/196] [Batch classify loss: 0.310756]
[Epoch 2/40] [Batch 60/196] [Batch classify loss: 0.689684]
[Epoch 2/40] [Batch 61/196] [Batch classify loss: 0.362825]
[Epoch 2/40] [Batch 62/196] [Batch classify loss: 1.075493]
[Epoch 2/40] [Batch 63/196] [Batch classify loss: 0.363602]
[Epoch 2/40] [Batch 64/196] [Batch classify loss: 0.864502]
[Epoch 2/40] [Batch 65/196] [Batch classify loss: 0.383077]
[Epoch 2/40] [Batch 66/196] [Batch classify loss: 0.926433]
[Epoch 2/40] [Batch 67/196] [Batch classify loss: 0.868748]
[Epoch 2/40] [Batch 68/196] [Batch classify loss: 0.622163]
[Epoch 2/40] [Batch 69/196] [Batch classify loss: 0.957937]
[Epoch 2/40] [Batch 70/196] [Batch classify loss: 0.374571]
[Epoch 2/40] [Batch 71/196] [Batch classify loss: 1.166903]
[Epoch 2/40] [Batch 72/196] [Batch classify loss: 0.485915]
[Epoch 2/40] [Batch 73/196] [Batch classify loss: 0.883062]
[Epoch 2/40] [Batch 74/196] [Batch classify loss: 0.430080]
[Epoch 2/40] [Batch 75/196] [Batch classify loss: 1.072672]
[Epoch 2/40] [Batch 76/196] [Batch classify loss: 0.453643]
[Epoch 2/40] [Batch 77/196] [Batch classify loss: 0.391889]
[Epoch 2/40] [Batch 78/196] [Batch classify loss: 0.338307]
[Epoch 2/40] [Batch 79/196] [Batch classify loss: 0.715185]
[Epoch 2/40] [Batch 80/196] [Batch classify loss: 1.063495]
[Epoch 2/40] [Batch 81/196] [Batch classify loss: 0.455298]
[Epoch 2/40] [Batch 82/196] [Batch classify loss: 0.509304]
[Epoch 2/40] [Batch 83/196] [Batch classify loss: 1.202472]
[Epoch 2/40] [Batch 84/196] [Batch classify loss: 0.568825]
[Epoch 2/40] [Batch 85/196] [Batch classify loss: 0.381043]
[Epoch 2/40] [Batch 86/196] [Batch classify loss: 0.640601]
[Epoch 2/40] [Batch 87/196] [Batch classify loss: 0.876657]
[Epoch 2/40] [Batch 88/196] [Batch classify loss: 0.306371]
[Epoch 2/40] [Batch 89/196] [Batch classify loss: 0.669091]
[Epoch 2/40] [Batch 90/196] [Batch classify loss: 0.582951]
[Epoch 2/40] [Batch 91/196] [Batch classify loss: 1.178801]
[Epoch 2/40] [Batch 92/196] [Batch classify loss: 1.103613]
[Epoch 2/40] [Batch 93/196] [Batch classify loss: 0.701183]
[Epoch 2/40] [Batch 94/196] [Batch classify loss: 0.447155]
[Epoch 2/40] [Batch 95/196] [Batch classify loss: 1.136916]
[Epoch 2/40] [Batch 96/196] [Batch classify loss: 0.453366]
[Epoch 2/40] [Batch 97/196] [Batch classify loss: 0.511975]
[Epoch 2/40] [Batch 98/196] [Batch classify loss: 0.613292]
[Epoch 2/40] [Batch 99/196] [Batch classify loss: 0.561735]
[Epoch 2/40] [Batch 100/196] [Batch classify loss: 0.468725]
[Epoch 2/40] [Batch 101/196] [Batch classify loss: 0.495612]
[Epoch 2/40] [Batch 102/196] [Batch classify loss: 1.067777]
[Epoch 2/40] [Batch 103/196] [Batch classify loss: 0.309297]
[Epoch 2/40] [Batch 104/196] [Batch classify loss: 0.323873]
[Epoch 2/40] [Batch 105/196] [Batch classify loss: 1.145155]
[Epoch 2/40] [Batch 106/196] [Batch classify loss: 0.344344]
[Epoch 2/40] [Batch 107/196] [Batch classify loss: 0.333514]
[Epoch 2/40] [Batch 108/196] [Batch classify loss: 0.360422]
[Epoch 2/40] [Batch 109/196] [Batch classify loss: 0.268697]
[Epoch 2/40] [Batch 110/196] [Batch classify loss: 1.191486]
[Epoch 2/40] [Batch 111/196] [Batch classify loss: 0.664459]
[Epoch 2/40] [Batch 112/196] [Batch classify loss: 0.333853]
[Epoch 2/40] [Batch 113/196] [Batch classify loss: 0.327806]
[Epoch 2/40] [Batch 114/196] [Batch classify loss: 0.376113]
[Epoch 2/40] [Batch 115/196] [Batch classify loss: 1.133879]
[Epoch 2/40] [Batch 116/196] [Batch classify loss: 0.361952]
[Epoch 2/40] [Batch 117/196] [Batch classify loss: 1.196431]
[Epoch 2/40] [Batch 118/196] [Batch classify loss: 0.858804]
[Epoch 2/40] [Batch 119/196] [Batch classify loss: 1.064665]
[Epoch 2/40] [Batch 120/196] [Batch classify loss: 1.125706]
[Epoch 2/40] [Batch 121/196] [Batch classify loss: 0.978642]
[Epoch 2/40] [Batch 122/196] [Batch classify loss: 0.428410]
[Epoch 2/40] [Batch 123/196] [Batch classify loss: 1.041231]
[Epoch 2/40] [Batch 124/196] [Batch classify loss: 0.380246]
[Epoch 2/40] [Batch 125/196] [Batch classify loss: 0.413532]
[Epoch 2/40] [Batch 126/196] [Batch classify loss: 0.866507]
[Epoch 2/40] [Batch 127/196] [Batch classify loss: 0.350390]
[Epoch 2/40] [Batch 128/196] [Batch classify loss: 0.386668]
[Epoch 2/40] [Batch 129/196] [Batch classify loss: 0.307194]
[Epoch 2/40] [Batch 130/196] [Batch classify loss: 1.180426]
[Epoch 2/40] [Batch 131/196] [Batch classify loss: 0.671005]
[Epoch 2/40] [Batch 132/196] [Batch classify loss: 0.876751]
[Epoch 2/40] [Batch 133/196] [Batch classify loss: 0.341272]
[Epoch 2/40] [Batch 134/196] [Batch classify loss: 0.345355]
[Epoch 2/40] [Batch 135/196] [Batch classify loss: 0.363582]
[Epoch 2/40] [Batch 136/196] [Batch classify loss: 0.335437]
[Epoch 2/40] [Batch 137/196] [Batch classify loss: 1.214646]
[Epoch 2/40] [Batch 138/196] [Batch classify loss: 0.762817]
[Epoch 2/40] [Batch 139/196] [Batch classify loss: 1.163934]
[Epoch 2/40] [Batch 140/196] [Batch classify loss: 1.142846]
[Epoch 2/40] [Batch 141/196] [Batch classify loss: 0.918513]
[Epoch 2/40] [Batch 142/196] [Batch classify loss: 0.546555]
[Epoch 2/40] [Batch 143/196] [Batch classify loss: 1.183027]
[Epoch 2/40] [Batch 144/196] [Batch classify loss: 0.433515]
[Epoch 2/40] [Batch 145/196] [Batch classify loss: 0.773001]
[Epoch 2/40] [Batch 146/196] [Batch classify loss: 1.132453]
[Epoch 2/40] [Batch 147/196] [Batch classify loss: 0.898957]
[Epoch 2/40] [Batch 148/196] [Batch classify loss: 0.318887]
[Epoch 2/40] [Batch 149/196] [Batch classify loss: 1.069793]
[Epoch 2/40] [Batch 150/196] [Batch classify loss: 0.523301]
[Epoch 2/40] [Batch 151/196] [Batch classify loss: 0.742212]
[Epoch 2/40] [Batch 152/196] [Batch classify loss: 0.396822]
[Epoch 2/40] [Batch 153/196] [Batch classify loss: 0.427195]
[Epoch 2/40] [Batch 154/196] [Batch classify loss: 1.233828]
[Epoch 2/40] [Batch 155/196] [Batch classify loss: 0.453425]
[Epoch 2/40] [Batch 156/196] [Batch classify loss: 0.998145]
[Epoch 2/40] [Batch 157/196] [Batch classify loss: 0.858254]
[Epoch 2/40] [Batch 158/196] [Batch classify loss: 1.137950]
[Epoch 2/40] [Batch 159/196] [Batch classify loss: 0.540287]
[Epoch 2/40] [Batch 160/196] [Batch classify loss: 0.630405]
[Epoch 2/40] [Batch 161/196] [Batch classify loss: 0.569451]
[Epoch 2/40] [Batch 162/196] [Batch classify loss: 0.633531]
[Epoch 2/40] [Batch 163/196] [Batch classify loss: 0.411208]
[Epoch 2/40] [Batch 164/196] [Batch classify loss: 0.342488]
[Epoch 2/40] [Batch 165/196] [Batch classify loss: 0.243048]
[Epoch 2/40] [Batch 166/196] [Batch classify loss: 0.646868]
[Epoch 2/40] [Batch 167/196] [Batch classify loss: 1.307074]
[Epoch 2/40] [Batch 168/196] [Batch classify loss: 0.895045]
[Epoch 2/40] [Batch 169/196] [Batch classify loss: 1.183660]
[Epoch 2/40] [Batch 170/196] [Batch classify loss: 1.181807]
[Epoch 2/40] [Batch 171/196] [Batch classify loss: 0.762031]
[Epoch 2/40] [Batch 172/196] [Batch classify loss: 0.419249]
[Epoch 2/40] [Batch 173/196] [Batch classify loss: 0.398476]
[Epoch 2/40] [Batch 174/196] [Batch classify loss: 0.971286]
[Epoch 2/40] [Batch 175/196] [Batch classify loss: 0.739382]
[Epoch 2/40] [Batch 176/196] [Batch classify loss: 0.818990]
[Epoch 2/40] [Batch 177/196] [Batch classify loss: 0.470004]
[Epoch 2/40] [Batch 178/196] [Batch classify loss: 0.579615]
[Epoch 2/40] [Batch 179/196] [Batch classify loss: 0.701246]
[Epoch 2/40] [Batch 180/196] [Batch classify loss: 0.821424]
[Epoch 2/40] [Batch 181/196] [Batch classify loss: 0.762699]
[Epoch 2/40] [Batch 182/196] [Batch classify loss: 0.963746]
[Epoch 2/40] [Batch 183/196] [Batch classify loss: 0.755796]
[Epoch 2/40] [Batch 184/196] [Batch classify loss: 1.069790]
[Epoch 2/40] [Batch 185/196] [Batch classify loss: 0.942533]
[Epoch 2/40] [Batch 186/196] [Batch classify loss: 0.781064]
[Epoch 2/40] [Batch 187/196] [Batch classify loss: 0.751809]
[Epoch 2/40] [Batch 188/196] [Batch classify loss: 1.083973]
[Epoch 2/40] [Batch 189/196] [Batch classify loss: 0.418541]
[Epoch 2/40] [Batch 190/196] [Batch classify loss: 0.664609]
[Epoch 2/40] [Batch 191/196] [Batch classify loss: 0.469585]
[Epoch 2/40] [Batch 192/196] [Batch classify loss: 0.468824]
[Epoch 2/40] [Batch 193/196] [Batch classify loss: 0.382752]
[Epoch 2/40] [Batch 194/196] [Batch classify loss: 0.735442]
[Epoch 2/40] [Batch 195/196] [Batch classify loss: 0.689725]
[Epoch 2/40] [Batch 196/196] [Batch classify loss: 1.447132]
0002 epoch rmt trained classifier accuary on the clean testing examples:82.3900%
0002 epoch rmt trained classifier loss on the clean testing examples:0.5627
initlize attack classifier
generate pixel adversarial exampels
Get FGSM examples generate model
self._args.attack_eps: 0.3
generating testset adversarial examples...
finished generate testset adversarial examples !
0002 epoch rmt trained classifier accuary on adversarial testset:18.7700%
0002 epoch rmt trained classifier loss on adversarial testset:3.054615020751953


2epoch learning rate:0.001
[Epoch 3/40] [Batch 1/196] [Batch classify loss: 1.049609]
[Epoch 3/40] [Batch 2/196] [Batch classify loss: 0.439068]
[Epoch 3/40] [Batch 3/196] [Batch classify loss: 1.067361]
[Epoch 3/40] [Batch 4/196] [Batch classify loss: 1.016693]
[Epoch 3/40] [Batch 5/196] [Batch classify loss: 0.374316]
[Epoch 3/40] [Batch 6/196] [Batch classify loss: 0.860478]
[Epoch 3/40] [Batch 7/196] [Batch classify loss: 0.692698]
[Epoch 3/40] [Batch 8/196] [Batch classify loss: 0.316186]
[Epoch 3/40] [Batch 9/196] [Batch classify loss: 1.030498]
[Epoch 3/40] [Batch 10/196] [Batch classify loss: 0.414556]
[Epoch 3/40] [Batch 11/196] [Batch classify loss: 0.386852]
[Epoch 3/40] [Batch 12/196] [Batch classify loss: 0.508590]
[Epoch 3/40] [Batch 13/196] [Batch classify loss: 0.429798]
[Epoch 3/40] [Batch 14/196] [Batch classify loss: 0.363552]
[Epoch 3/40] [Batch 15/196] [Batch classify loss: 0.597099]
[Epoch 3/40] [Batch 16/196] [Batch classify loss: 0.549068]
[Epoch 3/40] [Batch 17/196] [Batch classify loss: 1.224123]
[Epoch 3/40] [Batch 18/196] [Batch classify loss: 0.770994]
[Epoch 3/40] [Batch 19/196] [Batch classify loss: 0.830521]
[Epoch 3/40] [Batch 20/196] [Batch classify loss: 0.881762]
[Epoch 3/40] [Batch 21/196] [Batch classify loss: 0.412287]
[Epoch 3/40] [Batch 22/196] [Batch classify loss: 0.646304]
[Epoch 3/40] [Batch 23/196] [Batch classify loss: 1.122502]
[Epoch 3/40] [Batch 24/196] [Batch classify loss: 0.272233]
[Epoch 3/40] [Batch 25/196] [Batch classify loss: 0.909209]
[Epoch 3/40] [Batch 26/196] [Batch classify loss: 0.476932]
[Epoch 3/40] [Batch 27/196] [Batch classify loss: 0.321678]
[Epoch 3/40] [Batch 28/196] [Batch classify loss: 0.380838]
[Epoch 3/40] [Batch 29/196] [Batch classify loss: 0.890221]
[Epoch 3/40] [Batch 30/196] [Batch classify loss: 0.407888]
[Epoch 3/40] [Batch 31/196] [Batch classify loss: 0.995514]
[Epoch 3/40] [Batch 32/196] [Batch classify loss: 0.407583]
[Epoch 3/40] [Batch 33/196] [Batch classify loss: 0.213209]
[Epoch 3/40] [Batch 34/196] [Batch classify loss: 0.321992]
[Epoch 3/40] [Batch 35/196] [Batch classify loss: 0.703325]
[Epoch 3/40] [Batch 36/196] [Batch classify loss: 1.203334]
[Epoch 3/40] [Batch 37/196] [Batch classify loss: 1.141758]
[Epoch 3/40] [Batch 38/196] [Batch classify loss: 0.243079]
[Epoch 3/40] [Batch 39/196] [Batch classify loss: 1.093370]
[Epoch 3/40] [Batch 40/196] [Batch classify loss: 1.174785]
[Epoch 3/40] [Batch 41/196] [Batch classify loss: 0.720141]
[Epoch 3/40] [Batch 42/196] [Batch classify loss: 0.938799]
[Epoch 3/40] [Batch 43/196] [Batch classify loss: 0.716645]
[Epoch 3/40] [Batch 44/196] [Batch classify loss: 0.725506]
[Epoch 3/40] [Batch 45/196] [Batch classify loss: 0.886614]
[Epoch 3/40] [Batch 46/196] [Batch classify loss: 1.052579]
[Epoch 3/40] [Batch 47/196] [Batch classify loss: 0.382511]
[Epoch 3/40] [Batch 48/196] [Batch classify loss: 0.267404]
[Epoch 3/40] [Batch 49/196] [Batch classify loss: 0.573103]
[Epoch 3/40] [Batch 50/196] [Batch classify loss: 0.304742]
[Epoch 3/40] [Batch 51/196] [Batch classify loss: 1.016785]
[Epoch 3/40] [Batch 52/196] [Batch classify loss: 0.231204]
[Epoch 3/40] [Batch 53/196] [Batch classify loss: 1.046872]
[Epoch 3/40] [Batch 54/196] [Batch classify loss: 0.807992]
[Epoch 3/40] [Batch 55/196] [Batch classify loss: 0.487798]
[Epoch 3/40] [Batch 56/196] [Batch classify loss: 0.256012]
[Epoch 3/40] [Batch 57/196] [Batch classify loss: 0.346323]
[Epoch 3/40] [Batch 58/196] [Batch classify loss: 1.275333]
[Epoch 3/40] [Batch 59/196] [Batch classify loss: 0.640382]
[Epoch 3/40] [Batch 60/196] [Batch classify loss: 1.060959]
[Epoch 3/40] [Batch 61/196] [Batch classify loss: 0.349499]
[Epoch 3/40] [Batch 62/196] [Batch classify loss: 0.340045]
[Epoch 3/40] [Batch 63/196] [Batch classify loss: 0.862219]
[Epoch 3/40] [Batch 64/196] [Batch classify loss: 0.332129]
[Epoch 3/40] [Batch 65/196] [Batch classify loss: 0.280293]
[Epoch 3/40] [Batch 66/196] [Batch classify loss: 1.149005]
[Epoch 3/40] [Batch 67/196] [Batch classify loss: 0.854816]
[Epoch 3/40] [Batch 68/196] [Batch classify loss: 0.244893]
[Epoch 3/40] [Batch 69/196] [Batch classify loss: 0.588225]
[Epoch 3/40] [Batch 70/196] [Batch classify loss: 1.042888]
[Epoch 3/40] [Batch 71/196] [Batch classify loss: 1.073547]
[Epoch 3/40] [Batch 72/196] [Batch classify loss: 0.346587]
[Epoch 3/40] [Batch 73/196] [Batch classify loss: 0.569849]
[Epoch 3/40] [Batch 74/196] [Batch classify loss: 1.124697]
[Epoch 3/40] [Batch 75/196] [Batch classify loss: 1.100529]
[Epoch 3/40] [Batch 76/196] [Batch classify loss: 0.385760]
[Epoch 3/40] [Batch 77/196] [Batch classify loss: 0.342443]
[Epoch 3/40] [Batch 78/196] [Batch classify loss: 0.424557]
[Epoch 3/40] [Batch 79/196] [Batch classify loss: 0.867457]
[Epoch 3/40] [Batch 80/196] [Batch classify loss: 1.069430]
[Epoch 3/40] [Batch 81/196] [Batch classify loss: 0.981307]
[Epoch 3/40] [Batch 82/196] [Batch classify loss: 0.455344]
[Epoch 3/40] [Batch 83/196] [Batch classify loss: 0.905129]
[Epoch 3/40] [Batch 84/196] [Batch classify loss: 0.341811]
[Epoch 3/40] [Batch 85/196] [Batch classify loss: 0.569468]
[Epoch 3/40] [Batch 86/196] [Batch classify loss: 0.267369]
[Epoch 3/40] [Batch 87/196] [Batch classify loss: 1.152051]
[Epoch 3/40] [Batch 88/196] [Batch classify loss: 1.009164]
[Epoch 3/40] [Batch 89/196] [Batch classify loss: 0.537706]
[Epoch 3/40] [Batch 90/196] [Batch classify loss: 0.962857]
[Epoch 3/40] [Batch 91/196] [Batch classify loss: 1.075926]
[Epoch 3/40] [Batch 92/196] [Batch classify loss: 0.953738]
[Epoch 3/40] [Batch 93/196] [Batch classify loss: 0.827439]
[Epoch 3/40] [Batch 94/196] [Batch classify loss: 1.083886]
[Epoch 3/40] [Batch 95/196] [Batch classify loss: 0.275104]
[Epoch 3/40] [Batch 96/196] [Batch classify loss: 0.829497]
[Epoch 3/40] [Batch 97/196] [Batch classify loss: 0.738585]
[Epoch 3/40] [Batch 98/196] [Batch classify loss: 0.689845]
[Epoch 3/40] [Batch 99/196] [Batch classify loss: 1.044598]
[Epoch 3/40] [Batch 100/196] [Batch classify loss: 0.322019]
[Epoch 3/40] [Batch 101/196] [Batch classify loss: 0.700338]
[Epoch 3/40] [Batch 102/196] [Batch classify loss: 0.799921]
[Epoch 3/40] [Batch 103/196] [Batch classify loss: 0.429800]
[Epoch 3/40] [Batch 104/196] [Batch classify loss: 0.261423]
[Epoch 3/40] [Batch 105/196] [Batch classify loss: 0.953564]
[Epoch 3/40] [Batch 106/196] [Batch classify loss: 0.428078]
[Epoch 3/40] [Batch 107/196] [Batch classify loss: 0.554555]
[Epoch 3/40] [Batch 108/196] [Batch classify loss: 0.234247]
[Epoch 3/40] [Batch 109/196] [Batch classify loss: 0.949213]
[Epoch 3/40] [Batch 110/196] [Batch classify loss: 0.368438]
[Epoch 3/40] [Batch 111/196] [Batch classify loss: 1.063835]
[Epoch 3/40] [Batch 112/196] [Batch classify loss: 0.403758]
[Epoch 3/40] [Batch 113/196] [Batch classify loss: 0.977742]
[Epoch 3/40] [Batch 114/196] [Batch classify loss: 0.274021]
[Epoch 3/40] [Batch 115/196] [Batch classify loss: 0.314535]
[Epoch 3/40] [Batch 116/196] [Batch classify loss: 0.962218]
[Epoch 3/40] [Batch 117/196] [Batch classify loss: 1.009123]
[Epoch 3/40] [Batch 118/196] [Batch classify loss: 1.042097]
[Epoch 3/40] [Batch 119/196] [Batch classify loss: 0.484921]
[Epoch 3/40] [Batch 120/196] [Batch classify loss: 0.388927]
[Epoch 3/40] [Batch 121/196] [Batch classify loss: 0.948124]
[Epoch 3/40] [Batch 122/196] [Batch classify loss: 0.472246]
[Epoch 3/40] [Batch 123/196] [Batch classify loss: 0.255062]
[Epoch 3/40] [Batch 124/196] [Batch classify loss: 0.502411]
[Epoch 3/40] [Batch 125/196] [Batch classify loss: 0.263739]
[Epoch 3/40] [Batch 126/196] [Batch classify loss: 1.004141]
[Epoch 3/40] [Batch 127/196] [Batch classify loss: 0.339938]
[Epoch 3/40] [Batch 128/196] [Batch classify loss: 0.210451]
[Epoch 3/40] [Batch 129/196] [Batch classify loss: 0.211839]
[Epoch 3/40] [Batch 130/196] [Batch classify loss: 0.307929]
[Epoch 3/40] [Batch 131/196] [Batch classify loss: 0.844662]
[Epoch 3/40] [Batch 132/196] [Batch classify loss: 0.417631]
[Epoch 3/40] [Batch 133/196] [Batch classify loss: 0.996634]
[Epoch 3/40] [Batch 134/196] [Batch classify loss: 0.303752]
[Epoch 3/40] [Batch 135/196] [Batch classify loss: 0.754564]
[Epoch 3/40] [Batch 136/196] [Batch classify loss: 0.725394]
[Epoch 3/40] [Batch 137/196] [Batch classify loss: 0.921453]
[Epoch 3/40] [Batch 138/196] [Batch classify loss: 1.129112]
[Epoch 3/40] [Batch 139/196] [Batch classify loss: 1.122257]
[Epoch 3/40] [Batch 140/196] [Batch classify loss: 0.987357]
[Epoch 3/40] [Batch 141/196] [Batch classify loss: 0.942870]
[Epoch 3/40] [Batch 142/196] [Batch classify loss: 1.038446]
[Epoch 3/40] [Batch 143/196] [Batch classify loss: 0.383579]
[Epoch 3/40] [Batch 144/196] [Batch classify loss: 0.556383]
[Epoch 3/40] [Batch 145/196] [Batch classify loss: 0.373336]
[Epoch 3/40] [Batch 146/196] [Batch classify loss: 0.316759]
[Epoch 3/40] [Batch 147/196] [Batch classify loss: 1.128987]
[Epoch 3/40] [Batch 148/196] [Batch classify loss: 0.986893]
[Epoch 3/40] [Batch 149/196] [Batch classify loss: 0.438449]
[Epoch 3/40] [Batch 150/196] [Batch classify loss: 0.575396]
[Epoch 3/40] [Batch 151/196] [Batch classify loss: 0.992165]
[Epoch 3/40] [Batch 152/196] [Batch classify loss: 1.079908]
[Epoch 3/40] [Batch 153/196] [Batch classify loss: 0.515652]
[Epoch 3/40] [Batch 154/196] [Batch classify loss: 0.446936]
[Epoch 3/40] [Batch 155/196] [Batch classify loss: 0.355475]
[Epoch 3/40] [Batch 156/196] [Batch classify loss: 0.485327]
[Epoch 3/40] [Batch 157/196] [Batch classify loss: 0.894757]
[Epoch 3/40] [Batch 158/196] [Batch classify loss: 0.381116]
[Epoch 3/40] [Batch 159/196] [Batch classify loss: 0.524479]
[Epoch 3/40] [Batch 160/196] [Batch classify loss: 0.401585]
[Epoch 3/40] [Batch 161/196] [Batch classify loss: 0.611455]
[Epoch 3/40] [Batch 162/196] [Batch classify loss: 0.296434]
[Epoch 3/40] [Batch 163/196] [Batch classify loss: 0.969306]
[Epoch 3/40] [Batch 164/196] [Batch classify loss: 1.073889]
[Epoch 3/40] [Batch 165/196] [Batch classify loss: 0.423820]
[Epoch 3/40] [Batch 166/196] [Batch classify loss: 0.567217]
[Epoch 3/40] [Batch 167/196] [Batch classify loss: 1.019796]
[Epoch 3/40] [Batch 168/196] [Batch classify loss: 0.974415]
[Epoch 3/40] [Batch 169/196] [Batch classify loss: 0.278301]
[Epoch 3/40] [Batch 170/196] [Batch classify loss: 0.893038]
[Epoch 3/40] [Batch 171/196] [Batch classify loss: 0.418572]
[Epoch 3/40] [Batch 172/196] [Batch classify loss: 0.312569]
[Epoch 3/40] [Batch 173/196] [Batch classify loss: 0.782175]
[Epoch 3/40] [Batch 174/196] [Batch classify loss: 0.405498]
[Epoch 3/40] [Batch 175/196] [Batch classify loss: 0.447112]
[Epoch 3/40] [Batch 176/196] [Batch classify loss: 0.251636]
Traceback (most recent call last):
  File "tasklauncher-20220104.py", line 286, in <module>
    target_classifier.inputmixuptrain(args, cle_x_train, cle_y_train, cle_train_dataloader, cle_x_test,cle_y_test,adv_x_test,adv_y_test,exp_result_dir)
  File "/home/maggie/mmat/clamodels/classifier.py", line 2048, in inputmixuptrain
    self._optimizer.step()
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/optim/optimizer.py", line 89, in wrapper
    return func(*args, **kwargs)
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/optim/adam.py", line 119, in step
    group['eps'])
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/optim/_functional.py", line 92, in adam
    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
KeyboardInterrupt


---------------------------------------
Torch cuda is available
args.subcommand=run, run the command line
date: 20220104
whitebox attack
Experiment result save dir: /home/maggie/mmat/result/defense/inputmixup/fgsm/basemixup-betasampler/preactresnet34-cifar10/whitebox/20220104/00001
initilize the dataset loading parameters
load cifar10 dataset
Loading *cifar10* train dataloader finished !
Loading *cifar10* test dataloader finished !
initlize classifier
learned calssify model != None
cle_x_train.shape: torch.Size([25397, 3, 32, 32])
cle_y_train.shape: torch.Size([25397])
cle_y_train.shape: torch.Size([25397, 10])
args.adv_dataset： /home/maggie/mmat/result/attack/fgsm/preactresnet34-cifar10/20210927/00000-attackacc-18.04/attack-cifar10-dataset/samples
Accuary of before rmt trained classifier on clean testset:83.5700%
Loss of before mmat trained classifier clean testset:0.6700245141983032
Accuary of before rmt trained classifier on white-box adv testset:18.0400%
Loss of before rmt trained classifier on white-box adv testset:5.7243146896362305
args.mix_mode: basemixup
args.mix_w_num: 2
args.beta_alpha: 0.5
args.dirichlet_gama: 1
compare with---------input mixup train--------------
cle_x_train.shape: torch.Size([25397, 3, 32, 32])
cle_y_train.shape: torch.Size([25397, 10])
cle_x_test.shape: torch.Size([10000, 3, 32, 32])
cle_y_test.shape: torch.Size([10000])
adv_x_test.shape: torch.Size([10000, 3, 32, 32])
adv_y_test.shape: torch.Size([10000])
白盒
initlize attack classifier
generate pixel adversarial exampels
Get FGSM examples generate model
self._args.attack_eps: 0.3
generating testset adversarial examples...
finished generate testset adversarial examples !
Accuary of before rmt trained classifier on adversarial testset:18.0400%
Loss of before mmat trained classifier on adversarial testset:5.72437047958374
w_trainset_len: 25397
batch_size: 256
w_batch_num: 100


0epoch learning rate:0.001
[Epoch 1/40] [Batch 1/196] [Batch classify loss: 0.130008]
