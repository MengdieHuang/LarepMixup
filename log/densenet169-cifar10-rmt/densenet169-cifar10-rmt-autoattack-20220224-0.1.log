

---------------------------------------
Torch cuda is available
args.subcommand=run, run the command line
date: 20220224
blackbox attack
Experiment result save dir: /home/maggie/mmat/result/defense/rmt/autoattack/basemixup-betasampler/densenet169-cifar10/blackbox/20220224/00003
initilize the dataset loading parameters
load cifar10 dataset
Loading *cifar10* train dataloader finished !
Loading *cifar10* test dataloader finished !
Traceback (most recent call last):
  File "tasklauncher-20220218.py", line 198, in <module>
    learned_model = torch.load(args.cla_network_pkl)
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/serialization.py", line 592, in load
    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/serialization.py", line 851, in _load
    result = unpickler.load()
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/serialization.py", line 843, in persistent_load
    load_tensor(data_type, size, key, _maybe_decode_ascii(location))
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/serialization.py", line 832, in load_tensor
    loaded_storages[key] = restore_location(storage, location)
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/serialization.py", line 175, in default_restore_location
    result = fn(storage, location)
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/serialization.py", line 157, in _cuda_deserialize
    return obj.cuda(device)
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/_utils.py", line 80, in _cuda
    return new_type(self.size()).copy_(self, non_blocking)
  File "/home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/cuda/__init__.py", line 484, in _lazy_new
    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)
KeyboardInterrupt


---------------------------------------
Torch cuda is available
args.subcommand=run, run the command line
date: 20220224
blackbox attack
Experiment result save dir: /home/maggie/mmat/result/defense/rmt/autoattack/basemixup-betasampler/densenet169-cifar10/blackbox/20220224/00004
initilize the dataset loading parameters
load cifar10 dataset
Loading *cifar10* train dataloader finished !
Loading *cifar10* test dataloader finished !
initlize classifier
learned calssify model != None
args.adv_datasetï¼š /home/maggie/mmat/result/attack/autoattack/densenet169-cifar10/20220224/00000-eps0.1-acc3.96/attack-cifar10-dataset/samples
Accuary of before rmt trained classifier on clean testset:79.3600%
Loss of before mmat trained classifier clean testset:0.673821210861206
args.mix_mode: basemixup
args.mix_w_num: 2
args.beta_alpha: 2.0
args.dirichlet_gama: 1
cle_w_train.shape: torch.Size([25397, 8, 512])
cle_y_train.shape: torch.Size([25397, 8])
cle_x_test.shape: torch.Size([10000, 3, 32, 32])
cle_y_test.shape: torch.Size([10000])
adv_x_test.shape: torch.Size([10000, 3, 32, 32])
adv_y_test.shape: torch.Size([10000])
cle_train_dataloader.len: 196
Accuary of before rmt trained classifier on adversarial testset:3.9600%
Loss of before mmat trained classifier on adversarial testset:11.149736404418945
w_trainset_len: 25397
batch_size: 256
w_batch_num: 100


0epoch learning rate:0.001
Setting up PyTorch plugin "bias_act_plugin"... /home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/utils/cpp_extension.py:286: UserWarning: 

                               !! WARNING !!

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Your compiler (c++) is not compatible with the compiler Pytorch was
built with for this platform, which is g++ on linux. Please
use g++ to to compile your extension. Alternatively, you may
compile PyTorch from source using c++, and then you can also use
c++ to compile your extension.

See https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md for help
with compiling PyTorch from source.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

                              !! WARNING !!

  platform=sys.platform))
Done.
Setting up PyTorch plugin "upfirdn2d_plugin"... /home/xieyi/anaconda3/envs/mmat/lib/python3.7/site-packages/torch/utils/cpp_extension.py:286: UserWarning: 

                               !! WARNING !!

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Your compiler (c++) is not compatible with the compiler Pytorch was
built with for this platform, which is g++ on linux. Please
use g++ to to compile your extension. Alternatively, you may
compile PyTorch from source using c++, and then you can also use
c++ to compile your extension.

See https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md for help
with compiling PyTorch from source.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

                              !! WARNING !!

  platform=sys.platform))
Done.
[Epoch 1/40] [Batch 1/196] [Batch classify loss: 2.202580]
[Epoch 1/40] [Batch 2/196] [Batch classify loss: 2.053577]
[Epoch 1/40] [Batch 3/196] [Batch classify loss: 2.165667]
[Epoch 1/40] [Batch 4/196] [Batch classify loss: 2.139279]
[Epoch 1/40] [Batch 5/196] [Batch classify loss: 1.834884]
[Epoch 1/40] [Batch 6/196] [Batch classify loss: 1.938366]
[Epoch 1/40] [Batch 7/196] [Batch classify loss: 1.787084]
[Epoch 1/40] [Batch 8/196] [Batch classify loss: 1.930812]
[Epoch 1/40] [Batch 9/196] [Batch classify loss: 1.802429]
[Epoch 1/40] [Batch 10/196] [Batch classify loss: 1.953196]
[Epoch 1/40] [Batch 11/196] [Batch classify loss: 1.827385]
[Epoch 1/40] [Batch 12/196] [Batch classify loss: 1.740449]
[Epoch 1/40] [Batch 13/196] [Batch classify loss: 1.624604]
[Epoch 1/40] [Batch 14/196] [Batch classify loss: 1.858354]
[Epoch 1/40] [Batch 15/196] [Batch classify loss: 1.751637]
[Epoch 1/40] [Batch 16/196] [Batch classify loss: 1.731700]
[Epoch 1/40] [Batch 17/196] [Batch classify loss: 1.736396]
[Epoch 1/40] [Batch 18/196] [Batch classify loss: 1.564842]
[Epoch 1/40] [Batch 19/196] [Batch classify loss: 1.551010]
[Epoch 1/40] [Batch 20/196] [Batch classify loss: 1.541187]
[Epoch 1/40] [Batch 21/196] [Batch classify loss: 1.544716]
[Epoch 1/40] [Batch 22/196] [Batch classify loss: 1.540532]
[Epoch 1/40] [Batch 23/196] [Batch classify loss: 1.596792]
[Epoch 1/40] [Batch 24/196] [Batch classify loss: 1.448564]
[Epoch 1/40] [Batch 25/196] [Batch classify loss: 1.505708]
[Epoch 1/40] [Batch 26/196] [Batch classify loss: 1.583158]
[Epoch 1/40] [Batch 27/196] [Batch classify loss: 1.608501]
[Epoch 1/40] [Batch 28/196] [Batch classify loss: 1.515472]
[Epoch 1/40] [Batch 29/196] [Batch classify loss: 1.332504]
[Epoch 1/40] [Batch 30/196] [Batch classify loss: 1.454722]
[Epoch 1/40] [Batch 31/196] [Batch classify loss: 1.451826]
[Epoch 1/40] [Batch 32/196] [Batch classify loss: 1.360454]
[Epoch 1/40] [Batch 33/196] [Batch classify loss: 1.436695]
[Epoch 1/40] [Batch 34/196] [Batch classify loss: 1.334456]
[Epoch 1/40] [Batch 35/196] [Batch classify loss: 1.365137]
[Epoch 1/40] [Batch 36/196] [Batch classify loss: 1.304403]
[Epoch 1/40] [Batch 37/196] [Batch classify loss: 1.266653]
[Epoch 1/40] [Batch 38/196] [Batch classify loss: 1.316121]
[Epoch 1/40] [Batch 39/196] [Batch classify loss: 1.326635]
[Epoch 1/40] [Batch 40/196] [Batch classify loss: 1.296955]
[Epoch 1/40] [Batch 41/196] [Batch classify loss: 1.377075]
[Epoch 1/40] [Batch 42/196] [Batch classify loss: 1.288311]
[Epoch 1/40] [Batch 43/196] [Batch classify loss: 1.318231]
[Epoch 1/40] [Batch 44/196] [Batch classify loss: 1.267960]
[Epoch 1/40] [Batch 45/196] [Batch classify loss: 1.269617]
[Epoch 1/40] [Batch 46/196] [Batch classify loss: 1.223733]
